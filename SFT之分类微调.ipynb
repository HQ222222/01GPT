{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 引言\n",
    "\n",
    "前面我们花了三节内容来介绍预训练，包括如何从零搭建、如何加速运算、如何分布式加速训练，本节开始我们将进入监督微调（SFT）阶段。\n",
    "\n",
    "常见语言模型的微调任务有两类，分类微调和指令微调。\n",
    "- 分类微调模型通常是一个专用模型，它只能用来进行特定分类标签的预测，例如：输入一封邮件文本，输出这封邮件是垃圾邮件、非垃圾邮件。\n",
    "- 指令微调模型通常是一个多任务模型，它可以同时在多个任务上表现良好，训练的难度也更大。\n",
    "\n",
    "在这两类微调任务中，分类微调更为专注单一任务的优化，它在处理特定应用场景（例如情感分析、主题分类或意图识别等任务时）时，能够表现出更高的准确率，同时对训练数据量的要求也更少。并且，由于是单一任务，分类微调训练时不需要指令，直接输入数据进行训练，如下所示：\n",
    "![分类微调](./img/11-2.jpg)\n",
    "\n",
    "> 注：分类微调在传统的机器学习中非常常见，例如经典的手写数字识别，只不过手写数字识别是对图片进行分类，我们这里是对文本进行分类。\n",
    "\n",
    "本节我们将以垃圾邮件检测这个任务为例，来介绍如何对预训练模型进行分类微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据准备\n",
    "\n",
    "#### 2.1 数据下载\n",
    "\n",
    "先下载数据集：\n",
    "```\n",
    "!curl -o ../../dataset/minigpt/smsspam.txt \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "```\n",
    "下载完后进行解压，数据集是一个文本文件，可以通过pandas库来预览下载后的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/data2/minigpt/dataset/sft/smsspam.txt\", sep='\\t', header=None, names=[\"label\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，数据集比较简单，每个数据只有两个信息：文本(text)和标签分类(label)，标签分类也只有两个类别：`ham`表示正常邮件，`spam`表示垃圾邮件。\n",
    "\n",
    "#### 2.2 数据处理\n",
    "\n",
    "下面，我们来查看下两个标签分类的数据分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，两种标签的数据分布是不均衡的，正常邮件ham的数据量远比垃圾邮件spam类别要多。这不利于模型的训练学习，需要对数据分布作平衡操作。\n",
    "\n",
    "数据平衡通常有两种方式：\n",
    "1. 对偏多的数据做欠采样；\n",
    "2. 对偏少的数据做过采样。\n",
    "\n",
    "我们本节主要是为了演示，所以采用比较简单的方式，对ham类别的数据进行欠采样，使之与spam类别的数据量相等，均为747条。\n",
    "> 注：如果想要进行过采样，可以参考这篇文章中介绍的方法：[over_sampling](https://imbalanced-learn.org/stable/over_sampling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "spam    747\n",
       "ham     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balance_dataset(df):\n",
    "    spam_set = df[df[\"label\"] == \"spam\"]\n",
    "    ham_sub_set = df[df[\"label\"] == \"ham\"].sample(spam_set.shape[0], random_state=123)\n",
    "    return pd.concat([spam_set, ham_sub_set])\n",
    "\n",
    "balanced_df = balance_dataset(df)\n",
    "balanced_df[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将字符串分类转换为数字分类0和1，便于模型预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    747\n",
       "0    747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['label'] = balanced_df['label'].map({'spam': 1, 'ham': 0})\n",
    "balanced_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们写一个切分函数`random_split`，采用`8:1:1`的比例，随机的将数据集分割为训练集、验证集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1195, 149, 150)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_split(df, train_ratio, eval_ratio):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    train_len = int(len(df) * train_ratio)\n",
    "    eval_len = train_len + int(len(df) * eval_ratio)\n",
    "    return df[:train_len], df[train_len: eval_len], df[eval_len:]\n",
    "\n",
    "train_df, eval_df, test_df = random_split(balanced_df, 0.8, 0.1)\n",
    "len(train_df), len(eval_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 数据加载器\n",
    "\n",
    "模型训练时一般都会使用`小批量梯度下降`的方法，这要求同一批训练数据必须具有相同的序列长度，我们可以使用前面训练的分词器，对前10条文本进行序列化查看下长度分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq 0 length: 42\n",
      "seq 1 length: 45\n",
      "seq 2 length: 11\n",
      "seq 3 length: 23\n",
      "seq 4 length: 5\n",
      "seq 5 length: 49\n",
      "seq 6 length: 53\n",
      "seq 7 length: 6\n",
      "seq 8 length: 79\n",
      "seq 9 length: 12\n"
     ]
    }
   ],
   "source": [
    "from transformers  import AutoTokenizer\n",
    "\n",
    "tokenizer_path = \"/data2/minigpt/models/tokenizer_v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)\n",
    "inputs = [tokenizer.encode(text) for text in train_df['text']]\n",
    "for i, v in enumerate(inputs[:10]):\n",
    "    print(f\"seq {i} length: {len(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，训练数据集中每条文本序列化后的长度基本都不相同。我们想要每个小批量都具有相同的长度，就只能有两种做法：\n",
    "1. 长序列变短：将一个数据集或小批量中的所有文本都切割成与最短文本的序列长度相同； \n",
    "2. 短序列变长：将一个数据集或小批量中的所有文本都填充到与最长文本的序列长度相同； \n",
    "\n",
    "通常情况下，第二种方法用的更多，它能很好保留每条文本的内容完整。下面是使用`<|endoftext|>`作为填充token的长度填充示例。\n",
    "![填充短序列](./img/11-5.jpg)\n",
    "\n",
    "下面自定义一个数据集类，来封装文本的序列化和长度填充逻辑，以pytorch中标准化的形式来提供数据集迭代功能，每次迭代时会自动调用`__getitem__`内置方法，返回一条输入序列和一个目标分类标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, raw_df, tokenizer, max_tokens=1024):\n",
    "        self.labels = raw_df['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        pad_token_id = tokenizer.unk_token_id\n",
    "        inputs = [tokenizer.encode(text)[:max_tokens] for text in raw_df['text']]\n",
    "        max_length = max([len(item) for item in inputs])\n",
    "        self.inputs = [item + [pad_token_id] * (max_length - len(item)) for item in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (torch.tensor(self.inputs[i], dtype=torch.int64), \n",
    "            torch.tensor(self.labels.iloc[i], dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将前面切割后的训练集、验证集、测试集分别用SpamDataset封装，得到`train_set`、`eval_set`、`test_set`三个标准数据集，并以此为基础为训练、验证、则试三个场景分别创建三个小批量数据加载器`train_loader`、`eval_loader`、`test_loader`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size = 8\n",
    "\n",
    "train_set = SpamDataset(train_df, tokenizer)\n",
    "eval_set = SpamDataset(eval_df, tokenizer)\n",
    "test_set = SpamDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "eval_loader = DataLoader(eval_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对DataLoader进行迭代时，会自动按照`batch_size`指定的小批量大小，一次性从`Dataset`中取多条数据（如下所示inputs和labels分别是8条）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1148,  3476,  2948,  ...,     0,     0,     0],\n",
       "         [ 8150,    68,  6409,  ...,     0,     0,     0],\n",
       "         [12203,  6527,  1017,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [17647,  2511,   276,  ...,     0,     0,     0],\n",
       "         [   40, 14387,    39,  ...,     0,     0,     0],\n",
       "         [   43,  2341,   290,  ...,     0,     0,     0]]),\n",
       " tensor([1, 0, 1, 1, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_loader))\n",
    "inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准备模型\n",
    "\n",
    "在这一部分，我们需要对之前预训练的模型的输出头进行改造，用二分类代替词表长度的多分类。\n",
    "\n",
    "#### 3.1 加载模型状态\n",
    "\n",
    "首先，加载之前预训练的模型，来看下模型的结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniGPT(\n",
       "  (token_emb): Embedding(32000, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (decode_layers): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (atten): FlashMultiHeadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (Wo): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=32000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "checkpoint_path = \"/data2/minigpt/models/20241015/checkpoint-180000.pth\"\n",
    "\n",
    "model = MiniGPT(GPTConfig(flash_attn=True))\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前的输出头`out_head`将嵌入维度768映射到32000（词表大小），但是在这个垃圾分类任务中我们只需要二个目标分类`0`和`1`，因此，我们需要自己定义一个二分类输出头并替换掉原来的`out_head`，如下所示。\n",
    "\n",
    "#### 3.2 自定义输出头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "classes = 2\n",
    "\n",
    "model.out_head = nn.Linear(in_features=model.out_head.in_features, out_features=classes)\n",
    "model.out_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理论上来讲，只需要训练新的输出头`out_head`，但业界一些实验表明，训练更多的层有助于提高性能。因此，我们将最后一个transformer block和连接它的layernorm设置为可训练，其它层设置为不可训练（效果如下图和代码所示）。\n",
    "![可训练头](./img/11-3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7088642, 109605890)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.decode_layers[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.out_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params, all_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 如上打印信息所示，需要训练的参数总共有708万，占所有参数109605890的比例约`6.47%`。\n",
    "\n",
    "#### 3.3 模型测试\n",
    "\n",
    "拿一个输入文本来测试二分类输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Do you have time?\"\n",
    "input_tokens = tokenizer.encode(input_text)\n",
    "inputs = torch.tensor(input_tokens).unsqueeze(0)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以看到，输入与之前并没有什么区别，是一个batch_size=1,seq_len=5的单条序列，输出与之前有所不同，变成了维度为2的二分类输出。\n",
    "\n",
    "根据因果注意力掩码的特性，每个token都只能看到此token本身以及当前位置之前的token，而最后一个token会是唯一一个能包含所有token上下文的向量，因此，我们只用最后一个token的向量来计算最终的输出分类结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4616, 0.5384]]), tensor([1]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "preds = torch.argmax(probs, dim=-1)\n",
    "probs, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在未经训练情况下，模型预测的分类为是1（垃圾邮件），但实际上这个示例文本`Do you have time?`与垃圾邮件并无关系，这正是我们要通过微调模型来改善的地方。 \n",
    "\n",
    "接下来，我们编写一个函数`calc_accuracy`，用来计算模型在指定数据集上的分类准确率。它通过收集正确预测的数据条数占比来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7731215953826904"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "labels = torch.tensor([0])\n",
    "loss = f.cross_entropy(outputs[:, -1, :], labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(model, dataloader, device):\n",
    "    num_datas = 0\n",
    "    num_corrects = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        preds = torch.argmax(probs, dim=-1)\n",
    "        num_datas += len(labels)\n",
    "        num_corrects += (preds == labels).sum().item()\n",
    "    return num_corrects / num_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算模型在验证数据集上的分类准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5637583892617449"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(model, eval_loader, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算模型在测试数据集上的分类准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5733333333333334"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(model, test_loader, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，未经过微调的模型，不论是在验证数据集还是在测试数据上准确率都不是很好，这也是我们需要通过微调模型来改善的地方。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 微调\n",
    "\n",
    "#### 4.1 训练代码准备\n",
    "在微调开始之前，我们需要编写几个函数，以实现单步训练、模型评估和主训练循环。\n",
    "\n",
    "单步训练和前面预训练阶段的实现基本是一致的，步骤基本固定为以下这几步：\n",
    "- 梯度清零\n",
    "- 模型推理\n",
    "- 损失计算\n",
    "- 反向传播\n",
    "- 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "def train_step(model, optimizer, X, Y):\n",
    "    # 清零梯度\n",
    "    optimizer.zero_grad()\n",
    "    # 模型调用收集logits\n",
    "    logits = model(X)\n",
    "    # print(f\"logits.shape: {logits.shape}, target.shape: {Y.shape}\")\n",
    "    # 计算损失\n",
    "    loss = f.cross_entropy(logits[:, -1, :], Y.flatten())\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "模型评估与预训练阶段也基本类似，统计模型在整个数据集上的平均损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (X, Y) in dataloader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = f.cross_entropy(logits[:, -1, :], Y.flatten())\n",
    "            total_loss += loss.item()\n",
    "    model.train()\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主训练方法的实现也与前面大同小异，对所有的数据训练`num_epochs`指定的轮数，并在每轮训练中跟踪训练损失和验证损失的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, eval_loader, device, num_epochs):\n",
    "    train_losses, eval_losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            train_loss = train_step(model, optimizer, inputs.to(device), targets.to(device))\n",
    "            train_losses.append(train_loss)\n",
    "        eval_loss = evaluate(model, eval_loader, device)\n",
    "        eval_losses.append(eval_loss)\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "\n",
    "    return train_losses, eval_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.3042, Eval Loss: 0.4855\n",
      "Epoch: 1, Train Loss: 0.0237, Eval Loss: 0.2542\n",
      "Epoch: 2, Train Loss: 0.2716, Eval Loss: 0.2975\n",
      "Epoch: 3, Train Loss: 0.0011, Eval Loss: 0.2413\n",
      "Epoch: 4, Train Loss: 0.0008, Eval Loss: 0.2194\n",
      "Epoch: 5, Train Loss: 0.7194, Eval Loss: 0.2193\n",
      "Epoch: 6, Train Loss: 0.0016, Eval Loss: 0.2332\n",
      "Epoch: 7, Train Loss: 0.2460, Eval Loss: 0.2788\n",
      "Epoch: 8, Train Loss: 0.0699, Eval Loss: 0.3474\n",
      "Epoch: 9, Train Loss: 0.0037, Eval Loss: 0.3063\n",
      "Epoch: 10, Train Loss: 0.0004, Eval Loss: 0.2577\n",
      "Epoch: 11, Train Loss: 0.0004, Eval Loss: 0.4328\n",
      "Epoch: 12, Train Loss: 0.0935, Eval Loss: 0.2229\n",
      "Epoch: 13, Train Loss: 0.0000, Eval Loss: 0.2417\n",
      "Epoch: 14, Train Loss: 0.0022, Eval Loss: 0.5061\n",
      "Epoch: 15, Train Loss: 0.0369, Eval Loss: 0.2403\n",
      "Epoch: 16, Train Loss: 0.0000, Eval Loss: 0.6432\n",
      "Epoch: 17, Train Loss: 0.0008, Eval Loss: 0.2384\n",
      "Epoch: 18, Train Loss: 0.0000, Eval Loss: 0.2711\n",
      "Epoch: 19, Train Loss: 0.0000, Eval Loss: 0.3409\n",
      "CPU times: user 1min 7s, sys: 647 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6058809161186218,\n",
       "  0.237406387925148,\n",
       "  0.13311883807182312,\n",
       "  0.17648182809352875,\n",
       "  0.19205309450626373,\n",
       "  0.8970337510108948,\n",
       "  0.26322537660598755,\n",
       "  0.09653875231742859,\n",
       "  0.3542109727859497,\n",
       "  0.2244066447019577,\n",
       "  0.19476215541362762,\n",
       "  0.24518541991710663,\n",
       "  0.3518633544445038,\n",
       "  0.53542560338974,\n",
       "  0.5409538149833679,\n",
       "  0.5973795652389526,\n",
       "  0.458773672580719,\n",
       "  0.16411130130290985,\n",
       "  0.2323414385318756,\n",
       "  0.7260440587997437,\n",
       "  0.13508760929107666,\n",
       "  0.18483245372772217,\n",
       "  0.17834977805614471,\n",
       "  0.10417338460683823,\n",
       "  0.24924775958061218,\n",
       "  0.03430371731519699,\n",
       "  0.18500572443008423,\n",
       "  0.055364515632390976,\n",
       "  0.21246258914470673,\n",
       "  0.17938360571861267,\n",
       "  0.18209634721279144,\n",
       "  0.05074281617999077,\n",
       "  0.05779498815536499,\n",
       "  0.6211548447608948,\n",
       "  0.35117244720458984,\n",
       "  0.22149056196212769,\n",
       "  0.32643020153045654,\n",
       "  0.2309810370206833,\n",
       "  0.32617896795272827,\n",
       "  0.03717987611889839,\n",
       "  0.2025374472141266,\n",
       "  0.21727503836154938,\n",
       "  0.18198032677173615,\n",
       "  0.2953076660633087,\n",
       "  0.9433139562606812,\n",
       "  0.1267395317554474,\n",
       "  0.26642152667045593,\n",
       "  0.40004411339759827,\n",
       "  0.2579535245895386,\n",
       "  0.08940771222114563,\n",
       "  0.04034127667546272,\n",
       "  0.09557631611824036,\n",
       "  0.1733146607875824,\n",
       "  0.03556028753519058,\n",
       "  0.2729134261608124,\n",
       "  0.332750141620636,\n",
       "  0.22979110479354858,\n",
       "  0.05753684043884277,\n",
       "  0.2107115089893341,\n",
       "  0.11715252697467804,\n",
       "  0.09686902165412903,\n",
       "  0.27994850277900696,\n",
       "  0.1899668425321579,\n",
       "  0.07831420004367828,\n",
       "  0.06101680174469948,\n",
       "  0.09826832264661789,\n",
       "  0.09997404366731644,\n",
       "  0.39209112524986267,\n",
       "  0.23236584663391113,\n",
       "  0.15985120832920074,\n",
       "  0.05683865770697594,\n",
       "  0.3020503520965576,\n",
       "  0.17504802346229553,\n",
       "  0.2258523553609848,\n",
       "  0.15800170600414276,\n",
       "  0.39130324125289917,\n",
       "  0.21868710219860077,\n",
       "  0.08227299153804779,\n",
       "  0.40432432293891907,\n",
       "  0.2225770801305771,\n",
       "  0.2795564830303192,\n",
       "  0.08636785298585892,\n",
       "  0.07746052742004395,\n",
       "  0.06712013483047485,\n",
       "  0.21775655448436737,\n",
       "  0.3777772784233093,\n",
       "  0.07507383823394775,\n",
       "  0.03962099179625511,\n",
       "  0.38659319281578064,\n",
       "  0.042587678879499435,\n",
       "  0.4016595184803009,\n",
       "  0.13976965844631195,\n",
       "  0.12942056357860565,\n",
       "  0.5971888303756714,\n",
       "  0.286180704832077,\n",
       "  0.31258124113082886,\n",
       "  0.16268236935138702,\n",
       "  0.06888802349567413,\n",
       "  0.42895829677581787,\n",
       "  0.06956247240304947,\n",
       "  0.15529881417751312,\n",
       "  0.1423894166946411,\n",
       "  0.13395074009895325,\n",
       "  0.1511976569890976,\n",
       "  0.03617963567376137,\n",
       "  0.3861966133117676,\n",
       "  0.10323991626501083,\n",
       "  0.16733478009700775,\n",
       "  0.30573996901512146,\n",
       "  0.3994104564189911,\n",
       "  0.3700329065322876,\n",
       "  0.1389898657798767,\n",
       "  0.12263774871826172,\n",
       "  0.2215990126132965,\n",
       "  0.520727813243866,\n",
       "  0.07376135140657425,\n",
       "  0.6915712952613831,\n",
       "  0.48873192071914673,\n",
       "  0.594440758228302,\n",
       "  0.13548949360847473,\n",
       "  0.08535414934158325,\n",
       "  0.11311778426170349,\n",
       "  0.1248718872666359,\n",
       "  0.13781720399856567,\n",
       "  0.18597261607646942,\n",
       "  0.10070479661226273,\n",
       "  0.20392565429210663,\n",
       "  0.4904475808143616,\n",
       "  0.33162999153137207,\n",
       "  0.22453473508358002,\n",
       "  0.10672920942306519,\n",
       "  0.08193136751651764,\n",
       "  0.23729750514030457,\n",
       "  0.07115171104669571,\n",
       "  0.2946372926235199,\n",
       "  0.5781486630439758,\n",
       "  0.11101846396923065,\n",
       "  0.06260088086128235,\n",
       "  0.03988846391439438,\n",
       "  0.26685264706611633,\n",
       "  0.18733274936676025,\n",
       "  0.2501507103443146,\n",
       "  0.2177199125289917,\n",
       "  0.051370881497859955,\n",
       "  0.08488491177558899,\n",
       "  0.05740705132484436,\n",
       "  0.10370650142431259,\n",
       "  0.10100745409727097,\n",
       "  0.04925038293004036,\n",
       "  0.30423396825790405,\n",
       "  0.14411450922489166,\n",
       "  0.11984274536371231,\n",
       "  0.35949718952178955,\n",
       "  0.5084894299507141,\n",
       "  0.5090218782424927,\n",
       "  0.11668992787599564,\n",
       "  0.10303552448749542,\n",
       "  0.1957608014345169,\n",
       "  0.038869842886924744,\n",
       "  0.48875147104263306,\n",
       "  0.03954566270112991,\n",
       "  0.41308873891830444,\n",
       "  0.6016268134117126,\n",
       "  0.06092623248696327,\n",
       "  0.0644351989030838,\n",
       "  0.16752605140209198,\n",
       "  0.08462701737880707,\n",
       "  0.18799638748168945,\n",
       "  0.2241624891757965,\n",
       "  0.021846970543265343,\n",
       "  0.1744779646396637,\n",
       "  0.05199802666902542,\n",
       "  0.11073735356330872,\n",
       "  0.06737390905618668,\n",
       "  0.0685310885310173,\n",
       "  0.06199858337640762,\n",
       "  0.10422193259000778,\n",
       "  0.024140233173966408,\n",
       "  0.23358415067195892,\n",
       "  0.017975272610783577,\n",
       "  0.03396422415971756,\n",
       "  0.07196615636348724,\n",
       "  0.21187859773635864,\n",
       "  0.03651057928800583,\n",
       "  0.03493973985314369,\n",
       "  0.0912495031952858,\n",
       "  0.013056990690529346,\n",
       "  0.08944810926914215,\n",
       "  0.049044255167245865,\n",
       "  0.33245399594306946,\n",
       "  0.03386133909225464,\n",
       "  0.0070832096971571445,\n",
       "  0.19141659140586853,\n",
       "  0.06547950208187103,\n",
       "  0.33299747109413147,\n",
       "  0.6196173429489136,\n",
       "  0.1290690302848816,\n",
       "  0.7072442770004272,\n",
       "  0.08813879638910294,\n",
       "  0.13918755948543549,\n",
       "  0.1835300624370575,\n",
       "  0.9988305568695068,\n",
       "  0.033502332866191864,\n",
       "  0.475594162940979,\n",
       "  0.3880929946899414,\n",
       "  0.5172102451324463,\n",
       "  0.5252514481544495,\n",
       "  0.32519736886024475,\n",
       "  0.14309188723564148,\n",
       "  0.019814180210232735,\n",
       "  0.011628289707005024,\n",
       "  0.18835659325122833,\n",
       "  0.9725475907325745,\n",
       "  0.04431905597448349,\n",
       "  0.5307208895683289,\n",
       "  1.062110185623169,\n",
       "  0.49210071563720703,\n",
       "  0.5212368369102478,\n",
       "  0.03638973459601402,\n",
       "  0.8023989200592041,\n",
       "  0.39579474925994873,\n",
       "  0.35667526721954346,\n",
       "  0.04165681451559067,\n",
       "  0.055583465844392776,\n",
       "  0.05356793478131294,\n",
       "  0.0816599503159523,\n",
       "  0.19950833916664124,\n",
       "  0.0964277982711792,\n",
       "  0.3326619267463684,\n",
       "  0.19601380825042725,\n",
       "  0.12846623361110687,\n",
       "  0.28171342611312866,\n",
       "  0.496786504983902,\n",
       "  0.14763209223747253,\n",
       "  0.014308540150523186,\n",
       "  0.2371831238269806,\n",
       "  0.04958344250917435,\n",
       "  0.4201422929763794,\n",
       "  0.15208931267261505,\n",
       "  0.021458208560943604,\n",
       "  0.2914344370365143,\n",
       "  0.6766587495803833,\n",
       "  0.12209481000900269,\n",
       "  0.3358169496059418,\n",
       "  0.033378858119249344,\n",
       "  0.023361437022686005,\n",
       "  0.42219358682632446,\n",
       "  0.16464097797870636,\n",
       "  0.089109867811203,\n",
       "  0.14173249900341034,\n",
       "  0.11104032397270203,\n",
       "  0.04061694070696831,\n",
       "  0.4834936261177063,\n",
       "  0.34954068064689636,\n",
       "  0.045566946268081665,\n",
       "  0.06187261641025543,\n",
       "  0.11409468948841095,\n",
       "  0.0494559183716774,\n",
       "  0.059792228043079376,\n",
       "  0.07398248463869095,\n",
       "  0.06801807135343552,\n",
       "  0.0983884260058403,\n",
       "  0.09459083527326584,\n",
       "  0.0561877004802227,\n",
       "  0.10334421694278717,\n",
       "  0.023380622267723083,\n",
       "  0.13322573900222778,\n",
       "  0.03959108516573906,\n",
       "  0.030603257939219475,\n",
       "  0.13229875266551971,\n",
       "  0.2782677114009857,\n",
       "  0.015631990507245064,\n",
       "  0.07014499604701996,\n",
       "  0.10061433166265488,\n",
       "  0.03351129591464996,\n",
       "  0.7565436363220215,\n",
       "  0.033519528806209564,\n",
       "  0.24668575823307037,\n",
       "  0.010849461890757084,\n",
       "  0.11545465141534805,\n",
       "  0.09858017414808273,\n",
       "  0.2590944170951843,\n",
       "  0.37324637174606323,\n",
       "  0.0241946242749691,\n",
       "  0.3973589539527893,\n",
       "  0.06049998104572296,\n",
       "  0.0421551950275898,\n",
       "  0.005722083151340485,\n",
       "  0.4155556857585907,\n",
       "  0.03543628752231598,\n",
       "  0.2663923501968384,\n",
       "  0.04492022097110748,\n",
       "  0.22098790109157562,\n",
       "  0.3566741943359375,\n",
       "  0.020069129765033722,\n",
       "  0.05223723500967026,\n",
       "  0.29675155878067017,\n",
       "  0.010678929276764393,\n",
       "  0.16215543448925018,\n",
       "  0.023731432855129242,\n",
       "  0.03022846207022667,\n",
       "  0.10266971588134766,\n",
       "  0.15406177937984467,\n",
       "  0.06197791174054146,\n",
       "  0.17449122667312622,\n",
       "  0.20704807341098785,\n",
       "  0.1517365723848343,\n",
       "  0.15309372544288635,\n",
       "  0.0649673119187355,\n",
       "  0.0321681834757328,\n",
       "  0.3744804561138153,\n",
       "  0.009549474343657494,\n",
       "  0.26646432280540466,\n",
       "  0.05105794966220856,\n",
       "  0.018828339874744415,\n",
       "  0.007550867274403572,\n",
       "  0.08222122490406036,\n",
       "  0.08403453230857849,\n",
       "  0.006159096956253052,\n",
       "  0.024161869660019875,\n",
       "  0.0193362757563591,\n",
       "  0.05971124395728111,\n",
       "  0.15502920746803284,\n",
       "  0.15965893864631653,\n",
       "  0.01841297373175621,\n",
       "  0.10048206150531769,\n",
       "  0.15730680525302887,\n",
       "  0.2514931559562683,\n",
       "  0.10162608325481415,\n",
       "  0.008709048852324486,\n",
       "  0.07014555484056473,\n",
       "  0.14498814940452576,\n",
       "  0.11857283115386963,\n",
       "  0.39373502135276794,\n",
       "  0.19301725924015045,\n",
       "  0.1313769370317459,\n",
       "  0.027041565626859665,\n",
       "  0.035945065319538116,\n",
       "  0.013604712672531605,\n",
       "  0.2500569224357605,\n",
       "  0.008645249530673027,\n",
       "  0.011616352945566177,\n",
       "  0.27560025453567505,\n",
       "  0.03477819263935089,\n",
       "  0.0036459763068705797,\n",
       "  0.6914756298065186,\n",
       "  0.015201379545032978,\n",
       "  0.011804888024926186,\n",
       "  0.3836255669593811,\n",
       "  0.009726816788315773,\n",
       "  0.006340392865240574,\n",
       "  0.5235385894775391,\n",
       "  0.05395499989390373,\n",
       "  0.060086071491241455,\n",
       "  0.11716307699680328,\n",
       "  0.10689172893762589,\n",
       "  0.1235898807644844,\n",
       "  0.20781011879444122,\n",
       "  0.373322457075119,\n",
       "  0.03327183052897453,\n",
       "  0.16772671043872833,\n",
       "  0.23238231241703033,\n",
       "  0.1279992014169693,\n",
       "  0.024393673986196518,\n",
       "  0.2207605242729187,\n",
       "  0.020521169528365135,\n",
       "  0.031922418624162674,\n",
       "  0.06930463016033173,\n",
       "  0.29354503750801086,\n",
       "  0.021674776449799538,\n",
       "  0.0725797638297081,\n",
       "  0.12921413779258728,\n",
       "  0.02752598375082016,\n",
       "  0.20283620059490204,\n",
       "  0.18037806451320648,\n",
       "  0.02457534708082676,\n",
       "  0.25412341952323914,\n",
       "  0.37165963649749756,\n",
       "  0.18350479006767273,\n",
       "  0.03271586447954178,\n",
       "  0.042130544781684875,\n",
       "  0.05148304998874664,\n",
       "  0.11404914408922195,\n",
       "  0.04494386166334152,\n",
       "  0.027014397084712982,\n",
       "  0.021454617381095886,\n",
       "  0.5727132558822632,\n",
       "  0.5245381593704224,\n",
       "  0.0970149114727974,\n",
       "  0.22693665325641632,\n",
       "  0.12800708413124084,\n",
       "  0.00344184716232121,\n",
       "  0.010548093356192112,\n",
       "  0.04945098236203194,\n",
       "  0.08956922590732574,\n",
       "  0.004473256412893534,\n",
       "  0.025707298889756203,\n",
       "  0.3301038146018982,\n",
       "  0.022362347692251205,\n",
       "  0.013689828105270863,\n",
       "  0.09512841701507568,\n",
       "  0.18421922624111176,\n",
       "  0.023265797644853592,\n",
       "  0.4860633313655853,\n",
       "  0.0038218472618609667,\n",
       "  0.023656675592064857,\n",
       "  0.029001524671912193,\n",
       "  0.008259642869234085,\n",
       "  0.018740156665444374,\n",
       "  0.07145381718873978,\n",
       "  0.7049922943115234,\n",
       "  0.01382534485310316,\n",
       "  0.1421077996492386,\n",
       "  0.024947188794612885,\n",
       "  0.2656213939189911,\n",
       "  0.03540695458650589,\n",
       "  0.018482167273759842,\n",
       "  0.22601021826267242,\n",
       "  0.031223364174365997,\n",
       "  0.0452248677611351,\n",
       "  0.03986756503582001,\n",
       "  0.053134944289922714,\n",
       "  0.213278129696846,\n",
       "  0.5779345631599426,\n",
       "  0.02910466119647026,\n",
       "  0.09458497166633606,\n",
       "  0.3300268352031708,\n",
       "  0.021851394325494766,\n",
       "  0.05413946509361267,\n",
       "  0.08526822179555893,\n",
       "  0.18047787249088287,\n",
       "  0.1205047070980072,\n",
       "  0.04325132817029953,\n",
       "  0.30916550755500793,\n",
       "  0.07033059000968933,\n",
       "  0.045620765537023544,\n",
       "  0.02655893936753273,\n",
       "  0.38089194893836975,\n",
       "  0.03259533271193504,\n",
       "  0.004550641402602196,\n",
       "  0.13575276732444763,\n",
       "  0.012569994665682316,\n",
       "  0.7854418754577637,\n",
       "  0.3413144052028656,\n",
       "  0.00824501272290945,\n",
       "  0.4094095826148987,\n",
       "  0.2226671427488327,\n",
       "  0.18266785144805908,\n",
       "  0.1149701252579689,\n",
       "  0.2715877890586853,\n",
       "  0.010043269954621792,\n",
       "  0.26542556285858154,\n",
       "  0.38331708312034607,\n",
       "  0.008361779153347015,\n",
       "  0.03594828397035599,\n",
       "  0.004210733808577061,\n",
       "  0.058968156576156616,\n",
       "  0.3078036308288574,\n",
       "  0.043537333607673645,\n",
       "  0.1217387318611145,\n",
       "  0.07016557455062866,\n",
       "  0.2964917719364166,\n",
       "  0.019618332386016846,\n",
       "  0.05270371958613396,\n",
       "  0.06797593086957932,\n",
       "  0.14800795912742615,\n",
       "  0.006138921715319157,\n",
       "  0.10234878957271576,\n",
       "  0.07110130041837692,\n",
       "  0.7389637231826782,\n",
       "  0.009152551181614399,\n",
       "  0.0064186640083789825,\n",
       "  0.03349192440509796,\n",
       "  0.18911698460578918,\n",
       "  0.08440089225769043,\n",
       "  0.2638302743434906,\n",
       "  0.06376226991415024,\n",
       "  0.07944789528846741,\n",
       "  0.10323598980903625,\n",
       "  0.010589457117021084,\n",
       "  0.053927429020404816,\n",
       "  0.03453093767166138,\n",
       "  0.04731849581003189,\n",
       "  0.3349568247795105,\n",
       "  0.08885104954242706,\n",
       "  0.0837116688489914,\n",
       "  0.13979236781597137,\n",
       "  0.10300082713365555,\n",
       "  0.1221652403473854,\n",
       "  0.018571091815829277,\n",
       "  0.05783092603087425,\n",
       "  0.18164174258708954,\n",
       "  0.15417340397834778,\n",
       "  0.09455709159374237,\n",
       "  0.003184122731909156,\n",
       "  0.030143436044454575,\n",
       "  0.09162818640470505,\n",
       "  0.0773538276553154,\n",
       "  0.03787264600396156,\n",
       "  0.36373063921928406,\n",
       "  0.9693213701248169,\n",
       "  0.04189090430736542,\n",
       "  0.040112171322107315,\n",
       "  0.027431508526206017,\n",
       "  0.044481996446847916,\n",
       "  0.09457054734230042,\n",
       "  0.025697926059365273,\n",
       "  0.03205365315079689,\n",
       "  0.051174476742744446,\n",
       "  0.16200709342956543,\n",
       "  0.04961930587887764,\n",
       "  0.036015890538692474,\n",
       "  0.17455647885799408,\n",
       "  0.048477061092853546,\n",
       "  0.3517269194126129,\n",
       "  0.01886644959449768,\n",
       "  0.006563503295183182,\n",
       "  0.013141506351530552,\n",
       "  0.08550892770290375,\n",
       "  0.04000207409262657,\n",
       "  0.01133451983332634,\n",
       "  0.15131552517414093,\n",
       "  0.010958069004118443,\n",
       "  0.6298279166221619,\n",
       "  0.025729037821292877,\n",
       "  0.006275462452322245,\n",
       "  0.10096883773803711,\n",
       "  0.003548023058101535,\n",
       "  0.18955758213996887,\n",
       "  0.007927234284579754,\n",
       "  0.06494463235139847,\n",
       "  0.2658965587615967,\n",
       "  0.4476924538612366,\n",
       "  0.12569482624530792,\n",
       "  0.01368255726993084,\n",
       "  0.04204811900854111,\n",
       "  0.7370187640190125,\n",
       "  0.6804085969924927,\n",
       "  0.047295719385147095,\n",
       "  0.07103284448385239,\n",
       "  0.2646861672401428,\n",
       "  0.028081364929676056,\n",
       "  0.01041096355766058,\n",
       "  0.05189055576920509,\n",
       "  0.0014035701751708984,\n",
       "  0.023120561614632607,\n",
       "  0.002988783409819007,\n",
       "  0.0915251150727272,\n",
       "  0.015662197023630142,\n",
       "  0.003961079753935337,\n",
       "  0.002273102756589651,\n",
       "  0.29504290223121643,\n",
       "  0.005361609160900116,\n",
       "  0.004135137889534235,\n",
       "  0.3546755611896515,\n",
       "  0.015428927727043629,\n",
       "  0.7316227555274963,\n",
       "  0.6621348857879639,\n",
       "  0.20029640197753906,\n",
       "  0.01562832109630108,\n",
       "  0.02731999382376671,\n",
       "  0.3130234181880951,\n",
       "  0.04475351423025131,\n",
       "  0.024640990421175957,\n",
       "  0.07214293628931046,\n",
       "  0.0643736869096756,\n",
       "  0.06150022894144058,\n",
       "  0.32166001200675964,\n",
       "  0.10456647723913193,\n",
       "  0.08534957468509674,\n",
       "  0.21126940846443176,\n",
       "  0.21721893548965454,\n",
       "  0.08132040500640869,\n",
       "  0.11021801829338074,\n",
       "  0.010438252240419388,\n",
       "  0.08588238805532455,\n",
       "  0.05212470889091492,\n",
       "  0.24644087255001068,\n",
       "  0.03638028725981712,\n",
       "  0.1412607729434967,\n",
       "  0.04372793063521385,\n",
       "  0.3931095600128174,\n",
       "  0.006922491826117039,\n",
       "  0.04638991504907608,\n",
       "  0.05371345952153206,\n",
       "  0.009394078515470028,\n",
       "  0.00280447187833488,\n",
       "  0.06901812553405762,\n",
       "  0.012472571805119514,\n",
       "  0.002385219559073448,\n",
       "  0.4734756350517273,\n",
       "  0.05537821352481842,\n",
       "  0.045222003012895584,\n",
       "  0.006818796508014202,\n",
       "  0.15239083766937256,\n",
       "  0.12333898991346359,\n",
       "  0.30046048760414124,\n",
       "  0.32656118273735046,\n",
       "  0.10817556083202362,\n",
       "  0.0011274220887571573,\n",
       "  0.11332505196332932,\n",
       "  0.041315458714962006,\n",
       "  0.11419254541397095,\n",
       "  0.006916354410350323,\n",
       "  0.04375895857810974,\n",
       "  0.009598325937986374,\n",
       "  0.015304052270948887,\n",
       "  0.20173561573028564,\n",
       "  0.11173903942108154,\n",
       "  0.04938405379652977,\n",
       "  0.06867377460002899,\n",
       "  0.2469099909067154,\n",
       "  0.03653692081570625,\n",
       "  0.036099500954151154,\n",
       "  0.05729791894555092,\n",
       "  0.004587086848914623,\n",
       "  0.04624494910240173,\n",
       "  0.0009363576536998153,\n",
       "  0.0021074991673231125,\n",
       "  0.07318440079689026,\n",
       "  0.014191064983606339,\n",
       "  0.09447548538446426,\n",
       "  0.015873171389102936,\n",
       "  0.022629428654909134,\n",
       "  0.021406611427664757,\n",
       "  0.3451775014400482,\n",
       "  0.013822201639413834,\n",
       "  0.0043293023481965065,\n",
       "  0.45600876212120056,\n",
       "  0.08406717330217361,\n",
       "  0.0038018361665308475,\n",
       "  0.7619179487228394,\n",
       "  0.1864245980978012,\n",
       "  0.3269588351249695,\n",
       "  0.007626131176948547,\n",
       "  0.054830100387334824,\n",
       "  0.06449340283870697,\n",
       "  0.018063174560666084,\n",
       "  0.07192327082157135,\n",
       "  0.0867156907916069,\n",
       "  0.18576183915138245,\n",
       "  0.020781876519322395,\n",
       "  0.06557867676019669,\n",
       "  0.012897399254143238,\n",
       "  0.08303435146808624,\n",
       "  0.04523332789540291,\n",
       "  0.26267531514167786,\n",
       "  0.038878727704286575,\n",
       "  0.04394317790865898,\n",
       "  0.029140595346689224,\n",
       "  0.1281149983406067,\n",
       "  0.5105782747268677,\n",
       "  0.02159285359084606,\n",
       "  0.04533844813704491,\n",
       "  0.03142406418919563,\n",
       "  0.1441807597875595,\n",
       "  0.004724552854895592,\n",
       "  0.03769084811210632,\n",
       "  0.0277935229241848,\n",
       "  0.013769825920462608,\n",
       "  0.17353439331054688,\n",
       "  0.05616621673107147,\n",
       "  0.09881499409675598,\n",
       "  0.02904982678592205,\n",
       "  0.687160074710846,\n",
       "  0.09507057815790176,\n",
       "  0.014349386096000671,\n",
       "  0.04009678587317467,\n",
       "  0.001554050832055509,\n",
       "  0.47126322984695435,\n",
       "  0.009079833514988422,\n",
       "  0.025150887668132782,\n",
       "  0.0660322979092598,\n",
       "  0.14914771914482117,\n",
       "  0.01132900733500719,\n",
       "  0.07904096692800522,\n",
       "  0.031147979199886322,\n",
       "  0.01942475512623787,\n",
       "  0.24352428317070007,\n",
       "  0.05285703018307686,\n",
       "  0.0006658579222857952,\n",
       "  0.11128031462430954,\n",
       "  0.08876524120569229,\n",
       "  0.17056304216384888,\n",
       "  0.02557668276131153,\n",
       "  0.024358564987778664,\n",
       "  0.14782673120498657,\n",
       "  0.3426162600517273,\n",
       "  0.1739082634449005,\n",
       "  0.28661584854125977,\n",
       "  0.012381953187286854,\n",
       "  0.036514557898044586,\n",
       "  0.4425426721572876,\n",
       "  0.12437687069177628,\n",
       "  0.03962181881070137,\n",
       "  0.018716951832175255,\n",
       "  0.17394782602787018,\n",
       "  0.455560564994812,\n",
       "  0.055144187062978745,\n",
       "  0.018198493868112564,\n",
       "  0.07065603137016296,\n",
       "  0.002221285831183195,\n",
       "  0.017310697585344315,\n",
       "  0.031871166080236435,\n",
       "  0.004605179652571678,\n",
       "  0.06073226034641266,\n",
       "  0.08914744853973389,\n",
       "  0.1592198610305786,\n",
       "  0.00769205903634429,\n",
       "  0.16695818305015564,\n",
       "  0.006187465973198414,\n",
       "  0.24026760458946228,\n",
       "  0.01737532764673233,\n",
       "  0.03262875974178314,\n",
       "  0.07710160315036774,\n",
       "  0.01740807667374611,\n",
       "  0.30206555128097534,\n",
       "  0.00556132011115551,\n",
       "  0.02895309031009674,\n",
       "  0.051430460065603256,\n",
       "  0.010464400984346867,\n",
       "  0.024976316839456558,\n",
       "  0.05652957409620285,\n",
       "  0.01115478202700615,\n",
       "  0.028142858296632767,\n",
       "  0.3462449014186859,\n",
       "  0.0037648312281817198,\n",
       "  0.07399816066026688,\n",
       "  0.0561949722468853,\n",
       "  0.02426416054368019,\n",
       "  0.25129085779190063,\n",
       "  0.031484201550483704,\n",
       "  0.09267576038837433,\n",
       "  0.004553025588393211,\n",
       "  0.1846178025007248,\n",
       "  0.13089369237422943,\n",
       "  0.5479212403297424,\n",
       "  0.007461227476596832,\n",
       "  0.7903614640235901,\n",
       "  0.057191845029592514,\n",
       "  0.01416831649839878,\n",
       "  0.01304604485630989,\n",
       "  0.03053789958357811,\n",
       "  0.0018077511340379715,\n",
       "  0.36895835399627686,\n",
       "  0.011404644697904587,\n",
       "  0.12423887103796005,\n",
       "  0.06614664942026138,\n",
       "  0.014400431886315346,\n",
       "  0.0007827151566743851,\n",
       "  0.0178529005497694,\n",
       "  0.09113501757383347,\n",
       "  0.03715289384126663,\n",
       "  0.013999160379171371,\n",
       "  0.009160486981272697,\n",
       "  0.29499348998069763,\n",
       "  0.03137560188770294,\n",
       "  0.039496049284935,\n",
       "  0.07164452970027924,\n",
       "  0.021684587001800537,\n",
       "  0.14094416797161102,\n",
       "  0.06497764587402344,\n",
       "  0.02656775526702404,\n",
       "  0.0026303159538656473,\n",
       "  0.15993978083133698,\n",
       "  0.06768541038036346,\n",
       "  0.10994173586368561,\n",
       "  0.00952120404690504,\n",
       "  0.010467656888067722,\n",
       "  0.08937150239944458,\n",
       "  0.13482315838336945,\n",
       "  0.2648821771144867,\n",
       "  0.028779303655028343,\n",
       "  0.01652240939438343,\n",
       "  0.0021947536151856184,\n",
       "  0.01166156679391861,\n",
       "  0.09761244803667068,\n",
       "  0.01141017023473978,\n",
       "  0.06421733647584915,\n",
       "  0.10684135556221008,\n",
       "  0.04288182407617569,\n",
       "  0.0017765389056876302,\n",
       "  0.3548010289669037,\n",
       "  0.19513972103595734,\n",
       "  0.06260467320680618,\n",
       "  0.008744191378355026,\n",
       "  0.03731611743569374,\n",
       "  0.10543709993362427,\n",
       "  0.00420582527294755,\n",
       "  0.013741966336965561,\n",
       "  0.03842293098568916,\n",
       "  0.002104183891788125,\n",
       "  0.07833060622215271,\n",
       "  0.39348095655441284,\n",
       "  0.006761184893548489,\n",
       "  0.06655719131231308,\n",
       "  0.004188708029687405,\n",
       "  0.4750964343547821,\n",
       "  0.5403428673744202,\n",
       "  0.0018461456056684256,\n",
       "  0.0021653345320373774,\n",
       "  0.03615745157003403,\n",
       "  0.1745837777853012,\n",
       "  0.1882789134979248,\n",
       "  0.061648041009902954,\n",
       "  0.015498624183237553,\n",
       "  0.41416284441947937,\n",
       "  0.080276258289814,\n",
       "  0.0009509120136499405,\n",
       "  0.5703358054161072,\n",
       "  0.48324668407440186,\n",
       "  0.01808210276067257,\n",
       "  0.0661264955997467,\n",
       "  0.017060834914445877,\n",
       "  0.012406058609485626,\n",
       "  0.37790143489837646,\n",
       "  0.14493606984615326,\n",
       "  0.21648070216178894,\n",
       "  0.19232746958732605,\n",
       "  0.0017322697676718235,\n",
       "  0.04694127291440964,\n",
       "  0.028157519176602364,\n",
       "  0.015649845823645592,\n",
       "  0.015383845195174217,\n",
       "  0.012305023148655891,\n",
       "  0.03320940583944321,\n",
       "  0.001534552313387394,\n",
       "  0.034017134457826614,\n",
       "  0.028040600940585136,\n",
       "  0.050556909292936325,\n",
       "  0.04266577959060669,\n",
       "  0.011081761680543423,\n",
       "  0.007593855727463961,\n",
       "  0.018685299903154373,\n",
       "  0.14646892249584198,\n",
       "  0.007068595848977566,\n",
       "  0.024551894515752792,\n",
       "  0.008050859905779362,\n",
       "  0.02712906338274479,\n",
       "  0.025735177099704742,\n",
       "  0.026933159679174423,\n",
       "  0.0031683500856161118,\n",
       "  0.037599753588438034,\n",
       "  0.01302582211792469,\n",
       "  0.06103753298521042,\n",
       "  0.08610333502292633,\n",
       "  0.02054024115204811,\n",
       "  0.0802977979183197,\n",
       "  0.02491440437734127,\n",
       "  0.09479133039712906,\n",
       "  0.29018595814704895,\n",
       "  0.03983145207166672,\n",
       "  0.02850942686200142,\n",
       "  0.0386832095682621,\n",
       "  0.0047003645449876785,\n",
       "  0.04563286900520325,\n",
       "  0.00215687183663249,\n",
       "  0.025617586448788643,\n",
       "  0.022576821967959404,\n",
       "  0.0291907861828804,\n",
       "  0.00786604918539524,\n",
       "  0.007443916518241167,\n",
       "  0.08245617151260376,\n",
       "  0.0552140437066555,\n",
       "  0.00731066707521677,\n",
       "  0.294734388589859,\n",
       "  0.006213362794369459,\n",
       "  0.015294892713427544,\n",
       "  0.006166260689496994,\n",
       "  0.23983550071716309,\n",
       "  0.12207135558128357,\n",
       "  0.18538421392440796,\n",
       "  0.07822918891906738,\n",
       "  0.011392775923013687,\n",
       "  0.2332044243812561,\n",
       "  0.043821077793836594,\n",
       "  0.02448466420173645,\n",
       "  0.005793317221105099,\n",
       "  0.4277694523334503,\n",
       "  0.24147000908851624,\n",
       "  0.003006895072758198,\n",
       "  0.24593545496463776,\n",
       "  0.007295165676623583,\n",
       "  0.1845434159040451,\n",
       "  0.09955091774463654,\n",
       "  0.06657018512487411,\n",
       "  0.023784440010786057,\n",
       "  0.008560596965253353,\n",
       "  0.03030397929251194,\n",
       "  0.09888231754302979,\n",
       "  0.002879018895328045,\n",
       "  0.3778485357761383,\n",
       "  0.003290569642558694,\n",
       "  0.18413053452968597,\n",
       "  0.03598623350262642,\n",
       "  0.1404266506433487,\n",
       "  0.1264844536781311,\n",
       "  0.003939690068364143,\n",
       "  0.13354933261871338,\n",
       "  0.7193973660469055,\n",
       "  0.04559670388698578,\n",
       "  0.1198788657784462,\n",
       "  0.09936311841011047,\n",
       "  0.042162150144577026,\n",
       "  0.010019630193710327,\n",
       "  0.007188400253653526,\n",
       "  0.41166266798973083,\n",
       "  0.011942829936742783,\n",
       "  0.06369148194789886,\n",
       "  0.13830019533634186,\n",
       "  0.0035676381085067987,\n",
       "  0.10062853991985321,\n",
       "  0.017640521749854088,\n",
       "  0.008106939494609833,\n",
       "  0.03105558082461357,\n",
       "  0.13193050026893616,\n",
       "  0.0028085317462682724,\n",
       "  0.0005124399322085083,\n",
       "  0.09865296632051468,\n",
       "  0.0317707434296608,\n",
       "  0.11354721337556839,\n",
       "  0.13637405633926392,\n",
       "  0.006371795665472746,\n",
       "  0.007361354772001505,\n",
       "  0.07325336337089539,\n",
       "  0.012113492004573345,\n",
       "  0.020678486675024033,\n",
       "  0.014437607489526272,\n",
       "  0.0027998776640743017,\n",
       "  0.0019031792180612683,\n",
       "  0.007187367882579565,\n",
       "  0.0007843169150874019,\n",
       "  0.0014214457478374243,\n",
       "  0.036071449518203735,\n",
       "  0.0011564643355086446,\n",
       "  0.03279224783182144,\n",
       "  0.011005455628037453,\n",
       "  0.0028481052722781897,\n",
       "  0.2054210603237152,\n",
       "  0.45077523589134216,\n",
       "  0.5486501455307007,\n",
       "  0.006521116942167282,\n",
       "  0.028884530067443848,\n",
       "  0.027222109958529472,\n",
       "  0.015709448605775833,\n",
       "  0.0026469421572983265,\n",
       "  0.15657418966293335,\n",
       "  0.0016634332714602351,\n",
       "  0.023685043677687645,\n",
       "  0.024281742051243782,\n",
       "  0.003131143283098936,\n",
       "  0.1027473583817482,\n",
       "  0.3033696711063385,\n",
       "  0.017652180045843124,\n",
       "  0.035904575139284134,\n",
       "  0.12793703377246857,\n",
       "  0.02895764634013176,\n",
       "  0.002068357542157173,\n",
       "  0.0031050622928887606,\n",
       "  0.0005579954595305026,\n",
       "  0.01888120546936989,\n",
       "  0.1198728159070015,\n",
       "  0.11796554923057556,\n",
       "  0.030220936983823776,\n",
       "  0.023825570940971375,\n",
       "  0.007438014727085829,\n",
       "  0.2552573084831238,\n",
       "  0.0015708128921687603,\n",
       "  0.11438258737325668,\n",
       "  0.0023579434491693974,\n",
       "  0.0029974819626659155,\n",
       "  0.2277528941631317,\n",
       "  0.004200426395982504,\n",
       "  0.04499765858054161,\n",
       "  0.4006507694721222,\n",
       "  0.1403651237487793,\n",
       "  0.01031503826379776,\n",
       "  0.09683674573898315,\n",
       "  0.02508595585823059,\n",
       "  0.09204185009002686,\n",
       "  0.016893420368433,\n",
       "  0.15905767679214478,\n",
       "  0.1059502363204956,\n",
       "  0.06401880830526352,\n",
       "  0.7752447724342346,\n",
       "  0.009491970762610435,\n",
       "  0.0035678702406585217,\n",
       "  0.006231788545846939,\n",
       "  0.018227051943540573,\n",
       "  0.00749963941052556,\n",
       "  0.19216488301753998,\n",
       "  0.04596800357103348,\n",
       "  0.02987779676914215,\n",
       "  0.0035538822412490845,\n",
       "  0.041347697377204895,\n",
       "  0.0028439946472644806,\n",
       "  0.005307155195623636,\n",
       "  0.026075884699821472,\n",
       "  0.0010980621445924044,\n",
       "  0.009923749603331089,\n",
       "  ...],\n",
       " [0.4854838644203387,\n",
       "  0.25417827834424217,\n",
       "  0.2974571260182481,\n",
       "  0.24127968498750738,\n",
       "  0.21940532258074535,\n",
       "  0.2193171240781483,\n",
       "  0.2332474344140409,\n",
       "  0.2788041739320186,\n",
       "  0.34738949561908256,\n",
       "  0.3062561433103646,\n",
       "  0.2577034381898365,\n",
       "  0.4328255794243887,\n",
       "  0.2228802854461767,\n",
       "  0.24165687747197234,\n",
       "  0.5061312261657229,\n",
       "  0.2403018732860035,\n",
       "  0.643150046418764,\n",
       "  0.23836634182311842,\n",
       "  0.27108353591273426,\n",
       "  0.34086248057194196])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "device = 'cuda:0'\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "train(model, optimizer, train_loader, eval_loader, device, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注：从损失数据中可以看到，虽然训练了20轮，但跑到第6轮之后，就有些过拟合了，表现为训练损失很低，但验证损失不降反升的现象，虽然是我们的数据量太少，又重复训练了多轮的缘故。\n",
    "\n",
    "#### 4.3 准确率评估\n",
    "下面来统计下模型在不同数据集上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.999163179916318, eval_acc: 0.9328859060402684, test_acc: 0.9133333333333333\n"
     ]
    }
   ],
   "source": [
    "train_acc = calc_accuracy(model, train_loader, device)\n",
    "eval_acc = calc_accuracy(model, eval_loader, device)\n",
    "test_acc = calc_accuracy(model, test_loader, device)\n",
    "print(f\"train_acc: {train_acc}, eval_acc: {eval_acc}, test_acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果分析如下：\n",
    "- 在训练数据集准确率接近1.0，不太正常，原因在于训练轮数过多，过度拟合了训练数据集。\n",
    "- 在验证数据集上0.9328%，较为正常。\n",
    "- 在测试数据集上准确率为0.913，这是模型从未见过的数据集，这个数字最为客观。\n",
    "> 注：基于前面的损失下降数据，模型性能最好的点理论上应该是在epoch=5(eval_loss=0.2193)的时候，但是这个点的模型状态我们未能保存下来，这里主要为了演示，就没有再次训练，你如果感兴趣，可以修改代码将每个epoch的结果都保存下来，来找到准确率表现最佳的模型状态。\n",
    "\n",
    "下面我们将模型状态保存下来，以备后面使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = \"/data2/minigpt/models/classify/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"model_classify_v1.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 模型使用\n",
    "\n",
    "我们先编写一个`predict`函数，用于对文本进行垃圾邮件分类预测，并将结果转换为人类可以理解的文本形式`spam`和`not spam`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, device, max_length):\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "    probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "    pred = torch.argmax(probs, dim=-1)\n",
    "    return \"spam\" if pred.item() == 1 else \"not spam\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用刚微调的垃圾邮件分类模型，对下面两个样例文本进行实际预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a winner you have been specially  -->  not spam\n",
      "selected to receive $1000 cash or a $2000 award.  -->  spam\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"You are a winner you have been specially\",\n",
    "    \"selected to receive $1000 cash or a $2000 award.\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(text, \" --> \", predict(text, model, tokenizer, device, 131))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，对于这两个样例文本，在微调后的模型上分类均是正确的，基本达到了分类训练的目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小结**：本节我们主要介绍了基于预训练模型进行分类微调的实施过程，先对数据进行预处理并封装小批量数据加载器，再对模型结构进行改造，替换了一个二分类输出头，最后对模型进行微调训练，得到一个在测试数据集上准确率为91%的垃圾邮件分类模型，这个准确率应该还可以继续提高，有兴趣可以按照自己的想法动手修改验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
