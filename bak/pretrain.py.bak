import os
import math
import torch
import time
import torch.nn as nn
import torch.nn.functional as f
from contextlib import nullcontext
import torch.distributed as dist
from datetime import timedelta
from transformers import AutoTokenizer
from torch.utils.tensorboard import SummaryWriter
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler, random_split
from dataloader import PretrainSpeedupDataset
from transformer import GPTModel, MODEL_CONFIG, MiniGPTConfig, MiniGPT, generate_sequence

cur_time = lambda: time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time()))

def train_step(model, optimizer, scaler, X, Y):
    # 清零梯度, 设置为None不占用存储空间，对稀疏梯度会比较高效
    optimizer.zero_grad(set_to_none=True)
    # autocast可以在混合精度训练中自动转换精度，下面的nullcontext是为了兼容CPU设备，
    ctx = (nullcontext() 
        if X.device.type == "cpu" 
        else torch.amp.autocast('cuda'))
    
    # 下面将在前向传播和损失计算中自动使用 float16/bfloat16 精度。
    with ctx:
        # 模型调用收集logits
        logits = model(X)
        # 计算损失
        loss = f.cross_entropy(logits.flatten(0, 1), Y.flatten())
    
    # 反向传播计算梯度时，通过对梯度进行缩放来防止梯度下溢
    scaler.scale(loss).backward()
    # 梯度剪裁和更新参数之前，先取消梯度缩放，确保梯度在原始尺寸上进行处理
    scaler.unscale_(optimizer)
    # 对模型梯度进行裁剪，1.0是梯度最大范数，超过这个值的梯度都会被裁剪，防止梯度爆炸
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    # 更新模型参数
    scaler.step(optimizer)
    # 更新缩放器的缩放因子，以适应训练过程中数值变化
    scaler.update()
    return loss

def get_lr(target_lr, cur_step, warmup_steps, all_steps):
    min_lr = target_lr/10
    if cur_step < warmup_steps:
        return target_lr * (cur_step / warmup_steps)
    if cur_step > all_steps:
        return min_lr
    
    step_ratio = (cur_step - warmup_steps)/(all_steps-warmup_steps)
    cos_scope = 0.5 * (1 + math.cos(math.pi * step_ratio))
    return min_lr + (target_lr - min_lr) * cos_scope

def evaluate(model, dataloader, device):
    model = model.module if isinstance(model, DistributedDataParallel) else model
    model.eval()
    num_batches = len(dataloader)
    total_loss = 0
    for i, (X, Y) in enumerate(dataloader):
        with torch.no_grad():
            # print(f"{cur_time()}, before evaluate predict: {i}, X: {X.shape}, Y:{Y.shape}")
            logits = model(X.to(device))
            # print(f"{cur_time()}, after evaluate predict: {i}")
        loss = f.cross_entropy(logits.flatten(0, 1), Y.to(device).flatten())
        total_loss += loss.item()
    model.train()
    return total_loss/num_batches

def split_dataset(data, train_ratio, eval_ratio):
    train_len = int(len(data) * train_ratio)
    eval_len = int(len(data) * eval_ratio)
    test_len = len(data) - train_len - eval_len
    return random_split(data, [train_len, eval_len, test_len])

def create_dataloaders(ds, batch_size, local_rank, max_eval_data=1000):
    # 800万条数据，训练数据占99%，评估+测试约8万条数据
    train_set, eval_set, test_set = split_dataset(ds, 0.9900, 0.0002)
    sampler = DistributedSampler(train_set, rank=local_rank) if local_rank >= 0 else None
    shuffle = True if sampler == None else False
    # 用多进程num_workers加载，并不能提高速度。用于epoch结束后不关闭workers
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle, num_workers=0, drop_last=True, sampler=sampler)
    # 陷阱：这里的eval_set已经是被split_dataset切过的SubSet类型，它无法再次切割。
    eval_loader = DataLoader(eval_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)
    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)
    return train_loader, eval_loader, test_loader

def average_loss_across_gpus(loss):
    dist.reduce(loss, dst=0)
    return loss.item()/dist.get_world_size()

def record_train_data(writer, step, train_loss, eval_loss, grad_norm, lr):
    writer.add_scalars('loss', {'train': train_loss, 'eval': eval_loss}, step)
    writer.add_scalar("grad", grad_norm, step)
    writer.add_scalar("lr", lr, step)

def train_epoch_new(cur_epoch, start_step, train_loader, eval_loader, model, optimizer, scaler, writer, device, train_args:dict):
    start_time = time.time()
    train_loss_acc = 0   # 累积训练损失
    ddp = int(os.environ.get("RANK", -1)) != -1
    is_main_process = "cpu" in device or dist.get_rank() == 0
    num_epochs = train_args.get("num_train_epochs", 0)
    batch_size = train_args.get("train_batch_size", 8)
    eval_strategy = train_args.get("eval_strategy", "step")
    eval_steps = train_args.get("eval_steps", 1000)
    save_strategy = train_args.get("save_strategy", "step")
    save_steps = train_args.get("save_steps", 10000)
    warmup_steps = train_args.get("warmup_steps", 1000)
    output_dir = train_args.get("output_dir")
    target_lr = float(optimizer.defaults['lr'])
    print("target_lr:", target_lr)

    train_loader.sampler.set_epoch(cur_epoch)  # 每个epoch开始时都重新打乱数据
    steps_per_epoch = len(train_loader)
    global_step = cur_epoch * steps_per_epoch + start_step
    total_steps = num_epochs * steps_per_epoch
   
    # 每次迭代100万个字符的数据
    for step, (X, Y) in enumerate(train_loader):
        if step <= start_step: continue

        lr = get_lr(target_lr, global_step, warmup_steps, total_steps )
        if lr > 0:
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr

        
        loss = train_step(model, optimizer, scaler, X.to(device), Y.to(device))
        train_loss_acc += average_loss_across_gpus(loss) if ddp else loss.item()
        global_step += 1 
        torch.cuda.empty_cache()

        if eval_strategy == "step" and global_step % eval_steps == 0:
            train_loss = train_loss_acc/eval_steps
            if is_main_process:
                val_loss = evaluate(model, eval_loader, device)
                # print(f"{cur_time()} val_loss: {val_loss}")
                grad_norm = calc_grad_norm(model)
                record_train_data(writer, global_step, train_loss, val_loss, grad_norm, lr)
                train_loss_acc = 0
                spend_time = time.time() - start_time
                print(f"{cur_time()} {device}, lr={lr:.5f}, train_loss: {train_loss:.4f}, eval_loss: {val_loss:.4f}, grad_norm={grad_norm:.5f}, "
                    + f"{spend_time//60:.2f}min/{spend_time / (step - start_step + 1) * steps_per_epoch//60:.2f}min, "
                    + f"steps: {step}/{steps_per_epoch}"
                )
            # print(f"{cur_time()} {device} train_loss={train_loss}")
            dist.barrier()
            
        if save_strategy == "step" and global_step % save_steps == 0:
            if is_main_process:
                start_time_1 = time.time()
                checkpoint_path = f"{output_dir}/checkpoint-{global_step}.pth"
                save_model(model, optimizer, checkpoint_path, cur_epoch, step)
                print(f"{cur_time()} {dist.get_rank()}-save checkpoint: {checkpoint_path}, timestamp: {time.time() - start_time_1}s")
            
            dist.barrier()   # 保存checkpoint后设置屏障来同步所有进程
            print(f"{cur_time()} rank:{dist.get_rank()}, step: {step}/{global_step} after barrier.")

def init_distributed_mode():
    rank = int(os.environ.get("RANK", -1))
    
    ddp = False if rank == -1 else True
    if ddp == False: return False, -1, 'cpu'

    # os.environ['TORCH_NCCL_BLOCKING_WAIT'] = '1'
    # os.environ['TORCH_NCCL_ASYNC_ERROR_HANDLING'] = '1'
    os.environ['NCCL_DEBUG'] = 'WARN'
    # os.environ['NCCL_DEBUG_SUBSYS'] = 'ALL'

    world_size = int(os.environ["WORLD_SIZE"])
    # 这里的timeout没什么用，只是迟延了timeout中断时间而已，最主要还是在于为什么会引发watchdog timeout
    # 下次尝试异步保存checkpoint
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size, timeout=timedelta(seconds=120))
    local_rank = int(os.environ['LOCAL_RANK'])
    device = f'cuda:{local_rank}'
    torch.cuda.set_device(device)

    print(f"rank:{rank}, world_size:{world_size}, local_rank: {local_rank}")
    return ddp, local_rank, device

def cleanup():
    dist.destroy_process_group()
    print("clean multi process.")

def save_model(model, optimizer, checkpoint_path, epoch, step):
    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
    local_model = model.module if isinstance(model, DistributedDataParallel) else model
    torch.save({
        "model_state": local_model.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "epoch": epoch,
        "step": step,
    }, checkpoint_path)

def load_from_checkpoint(model, optimizer, checkpoint_path, device='cuda'):
    # 在分布式训练的多GPU环境中，map_location可以确保模型的参数和优化器的状态被加载到正确的GPU上，避免出现设备不匹配而报错。
    local_model = model.module if isinstance(model, DistributedDataParallel) else model
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    local_model.load_state_dict(checkpoint['model_state'])
    if optimizer != None:
        optimizer.load_state_dict(checkpoint['optimizer_state'])
    return checkpoint.get('epoch', 0), checkpoint.get('step', 0)

def calc_grad_norm(model):
    """ 计算梯度范数，梯度范数为所有参数平方和的平方根。先为每一层计算梯度范数，再计算所有层合在一起的梯度范数。"""
    total_norm = 0.0
    for name, p in model.named_parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    return total_norm ** 0.5

def regist_tensorboad(model, output_dir, train_loader, device):
    # 告诉tensorboard，要将日志记录到哪个文件夹
    writer = SummaryWriter(f"{output_dir}/log")
    # 取一个样例数据进行预测，目的是收集到模型的计算图
    x_sample, _ = next(iter(train_loader))
    writer.add_graph(model, input_to_model=x_sample.to(device))
    return writer

def main():
    epochs = 10
    max_tokens = 1024
    learning_rate = 5e-3
    batch_size = 10
    
    last_checkpoint_path = "/data2/minigpt/models/20241015/checkpoint-780000.pth"
    dataset_path = "/data2/minigpt/dataset/pretrain/mobvoi_seq_monkey_general_open_corpus.bin"
    # dataset_path = "/data2/minigpt/dataset/pretrain/texts_to_bin_test.bin"
    output_dir = "/data2/minigpt/models/20241015"
    # 进程分布式
    device_type = 'cuda' if torch.cuda.is_available() else 'cpu'
    ddp, local_rank, device = init_distributed_mode()
    print(f"{cur_time()} ddp: {ddp}, local_rank: {local_rank}, device: {device}")

    start_time = time.time()
    start_epoch, start_step = 0, 0
    # 数据加载要设置分布式采样器
    ds = PretrainSpeedupDataset(dataset_path, max_tokens)
    train_loader, eval_loader, _ = create_dataloaders(ds, batch_size, local_rank)
    print(f"{cur_time()} load dataset: {len(ds)}, train: {len(train_loader)}, eval: {len(eval_loader)}, use time: {time.time()-start_time:.2f}s")

    # 模型分布式, autocast会自动将float32绽放为float16（autocast不支持bfloat16），这里不用指定数据类型
    # model = GPTModel(MODEL_CONFIG).to(device)
    model = MiniGPT(MiniGPTConfig()).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scaler = torch.amp.GradScaler('cuda', enabled=True)
    # 注册tensorboard 写log对象
    writer = regist_tensorboad(model, output_dir, train_loader, device)

    if last_checkpoint_path != "":
        start_epoch, start_step = load_from_checkpoint(model, optimizer, last_checkpoint_path, device)
        start_epoch = start_epoch + 1 if start_step < 0 else start_epoch
        print(f"{cur_time()} load checkpoint[{start_epoch}，{start_step}]: {last_checkpoint_path}")

    if ddp:
        # 位置编码用的是复数，而nccl不支持复数形式，此变量并不要求在多进程中保持一致，所以暂时屏蔽对此变量的同步，
        model._ddp_params_and_buffers_to_ignore = {"pos_cis"}
        model = DistributedDataParallel(model, device_ids=[local_rank])
        print(f"{cur_time()} {local_rank},packaged model with DDP in device: {device}")

    train_args = {
        "train_batch_size": batch_size,
        "eval_strategy": "step",
        "eval_steps": 1000,
        "warmup_steps": 1000,
        "save_strategy": "step",
        "save_steps": 30000,
        "num_train_epochs": epochs,
        "output_dir": output_dir,
    }

    for epoch in range(start_epoch, epochs):
        step = start_step if start_epoch == epoch else -1
        train_epoch_new(epoch, step, train_loader, eval_loader, model, optimizer, scaler, writer, device, train_args)
        if not ddp or dist.get_rank() == 0 and train_args.get("save_strategy") == "epoch":
            model.eval()
            model = model.module if isinstance(model, DistributedDataParallel) else model
            checkpoint_path = f"{output_dir}/checkpoint-{epoch}.pth"
            save_model(model, optimizer, checkpoint_path, epoch, -1)
            model.train()
            print(f"{local_rank}-save checkpoint: {checkpoint_path}")

    writer.close()
    cleanup()


def generate():
    device='cuda:1'
    model = GPTModel(MODEL_CONFIG).to(device)
    checkpoint_path = "/data2/minigpt/models/20241015/checkpoint-780000.pth"
    load_from_checkpoint(model, None, checkpoint_path, device)
    
    tokenizer_path = "/data2/minigpt/models/tokenizer_v3"
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)

    input_text = "小丽: 你好，我是文毅斌，很高兴认识你。\n小美: 你好"
    # input_text = "库里在第三节上篮时被防守球员犯规，但裁判并未理会"
    inputs = torch.tensor([tokenizer.encode(input_text)]).to(device)
    response_ids = model.generate(inputs, 512, tokenizer.eos_token_id)
    print(tokenizer.decode(response_ids.squeeze(0), skip_special_tokens=False))


# torchrun --nproc_per_node 2 1-pretrain.py
# I/O
if __name__ == "__main__":
    # generate()
    main()

            