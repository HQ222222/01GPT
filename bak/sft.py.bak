import os
import math
import json
import torch
import time
import torch.nn as nn
import linecache
from functools import partial
import torch.nn.functional as f
from contextlib import nullcontext
import torch.distributed as dist
from datetime import timedelta
from transformers import AutoTokenizer
from torch.utils.tensorboard import SummaryWriter
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import Dataset, DataLoader, DistributedSampler, random_split
from transformer import GPTModel, MODEL_CONFIG, generate_sequence

cur_time = lambda: time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time()))

class SFTDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=1024):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def process(self, item):
        user_content = item['instruction'] + '\n' + item['input']
        assistant_content = item['output']
        messages = [
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": assistant_content},
        ]
        input_ids =  self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=False)
        return input_ids[:self.max_len]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, i):
        return self.process(self.data[i])



class JSONLDataset(Dataset):
    def __init__(self, jsonl_file_path, tokenizer, max_len=1024, max_lines=0):
        self.jsonl_file_path = jsonl_file_path
        self.tokenizer = tokenizer
        self.max_len = max_len
        start_time = time.time()
        if max_lines <= 0:
            # 计算文件中的总行数
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                self.total_lines = sum(1 for _ in f)
        else:
            self.total_lines = max_lines
        print(f"calculate lines[{self.total_lines}] use time: {time.time()-start_time:.2f}s")

    def __len__(self):
        return self.total_lines

    def process(self, item):
        messages = []
        for history_item in item.get("history", []):
            if len(history_item) < 2:
                continue
            messages.append({"role": "user", "content": history_item[0][:self.max_len//2]})
            messages.append({"role": "assistant", "content": history_item[1][:self.max_len//2]})
        
        user_content = item['instruction'] + '\n' + item['input']
        assistant_content = item['output']
        messages.append({"role": "user", "content": user_content})
        messages.append({"role": "assistant", "content": assistant_content})
        input_ids =  self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=False)
        return input_ids[:self.max_len]

    def __getitem__(self, idx):
        indexes = [idx] if isinstance(idx, int) else idx
        assert isinstance(indexes, list)

        # 使用 linecache 读取指定行
        lines = [linecache.getline(self.jsonl_file_path, i + 1).strip() for i in indexes]
        datas = [json.loads(line) for line in lines]
        inputs =  [self.process(data) for data in datas]
        return inputs[0] if isinstance(idx, int) else inputs

    def __getitems__(self, idx):
        return self.__getitem__(idx)

def find_sublist_index(main_list, sub_list):
    for i in range(len(main_list) - len(sub_list) + 1):
        if main_list[i:i + len(sub_list)] == sub_list:
            return i
    return -1

def calc_label(input_ids, pad_token_id, tokenizer):
    target_ids = input_ids[1:] + [pad_token_id]
    output_seperator = tokenizer("<|im_start|>assistant\n")['input_ids']
    output_start_index = find_sublist_index(target_ids, output_seperator)
    instruction_length = output_start_index + len(output_seperator)
    label_ids = [-100 if (item == pad_token_id or i < instruction_length) else item for i, item in enumerate(target_ids)]
    return label_ids

def collate(batch_inputs, pad_token_id, tokenizer, device='cpu'):
    assert isinstance(pad_token_id, int)
    
    max_length = max([len(item) for item in batch_inputs])
    # pad each sequence to max_length
    batch_padded = [item + [pad_token_id] * (max_length - len(item)) for item in batch_inputs]
    input_tensors = torch.tensor(batch_padded, dtype=torch.int64).to(device)

    attention_mask = torch.ones(input_tensors.shape, dtype=torch.int64).to(device)
    attention_mask = attention_mask.masked_fill(input_tensors == pad_token_id, 0)
    
    batch_targets = [calc_label(item, pad_token_id, tokenizer) for item in batch_padded]
    target_tensors = torch.tensor(batch_targets, dtype=torch.int64).to(device)
    return input_tensors, target_tensors, attention_mask

def train_step(model, optimizer, scaler, X, Y):
    # 清零梯度, 设置为None不占用存储空间，对稀疏梯度会比较高效
    optimizer.zero_grad(set_to_none=True)
    # autocast可以在混合精度训练中自动转换精度，下面的nullcontext是为了兼容CPU设备，
    ctx = (nullcontext() 
        if X.device.type == "cpu" 
        else torch.amp.autocast('cuda'))
    
    # 下面将在前向传播和损失计算中自动使用 float16/bfloat16 精度。
    with ctx:
        # 模型调用收集logits
        logits = model(X)
        # 计算损失
        loss = f.cross_entropy(logits.flatten(0, 1), Y.flatten())
    
    # 反向传播计算梯度时，通过对梯度进行缩放来防止梯度下溢
    # 如果model被DDP封装，会自动对损失进行多进程的allreduce操作
    scaler.scale(loss).backward()  
    # 梯度剪裁和更新参数之前，先取消梯度缩放，确保梯度在原始尺寸上进行处理
    scaler.unscale_(optimizer)
    # 对模型梯度进行裁剪，1.0是梯度最大范数，超过这个值的梯度都会被裁剪，防止梯度爆炸
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    # 更新模型参数
    scaler.step(optimizer)
    # 更新缩放器的缩放因子，以适应训练过程中数值变化
    scaler.update()
    
    return loss

def get_lr(target_lr, cur_step, all_steps):
    warmup_steps = 0
    min_lr = target_lr/10
    if cur_step < warmup_steps:
        return target_lr * (cur_step / warmup_steps)
    if cur_step > all_steps:
        return min_lr
    
    step_ratio = (cur_step - warmup_steps)/(all_steps-warmup_steps)
    cos_scope = 0.5 * (1 + math.cos(math.pi * step_ratio))
    return min_lr + (target_lr - min_lr) * cos_scope

def evaluate(model, dataloader, device):
    model.eval()
    num_batches = len(dataloader)
    total_loss = 0
    for (X, Y, _) in dataloader:
        with torch.no_grad():
            logits = model(X.to(device))
        loss = f.cross_entropy(logits.flatten(0, 1), Y.to(device).flatten())
        total_loss += loss.item()
    model.train()
    return total_loss/num_batches

global_steps = 0

def train_epoch(cur_epoch, start_step, train_loader, eval_loader, model, optimizer, scaler, output_dir, device, writer, train_args:dict):
    start_time = time.time()
    losses = []
    ddp = int(os.environ.get("RANK", -1)) != -1
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    is_main_process = "cpu" in device or (not ddp) or dist.get_rank() == 0
    batch_size = train_args.get("train_batch_size", 8)
    eval_strategy = train_args.get("eval_strategy", "step")
    eval_steps = train_args.get("eval_steps", 1000)
    save_strategy = train_args.get("save_strategy", "step")
    save_steps = train_args.get("save_steps", 10000)
    num_epochs = train_args.get("num_train_epochs", 1)
    target_lr = float(optimizer.defaults['lr'])
    steps_per_epoch = len(train_loader)
    global_steps = cur_epoch * steps_per_epoch + start_step
    print("start epoch: ", cur_epoch, ", start_step: ", start_step, ", num_epochs: ", num_epochs)

    # 每次迭代100万个字符的数据
    for step, (X, Y, _) in enumerate(train_loader):
        if step < start_step: continue

        lr = get_lr(target_lr, global_steps, steps_per_epoch * num_epochs )
        if lr > 0:
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr

        loss = train_step(model, optimizer, scaler, X.to(device), Y.to(device))
        global_steps += 1 
        
        if eval_strategy == "step" and global_steps % eval_steps == 0:
            dist.reduce(loss, dst=0)
            if is_main_process:
                # 平均损失
                train_loss = loss.item()/world_size
                evaluate_start_time = time.time()
                local_model = model.module if isinstance(model, DistributedDataParallel) else model
                val_loss = evaluate(local_model, eval_loader, device)
                losses.append((global_steps, train_loss, val_loss, lr))
                spend_time = time.time() - start_time
                # 将损失数据写到tensorboard
                writer.add_scalars('loss', {'train': train_loss, 'eval': val_loss}, global_steps)
                print(f"{cur_time()} {device}, lr={lr:.8f}, train_loss: {train_loss:.4f}, eval_loss: {val_loss:.4f}, "
                    + f"{spend_time/60:.2f}min/{spend_time / (step - start_step + 1) * steps_per_epoch/60:.2f}min, "
                    + f"evaluate use time: {time.time()-evaluate_start_time:.2f}s, steps: {global_steps}/{num_epochs * steps_per_epoch}"
                )
            # 所有进程等待rank 0运行完评估，目的是解决rank 0与其它进程nccl同步超时的错误
            # 错误原因在于：上面的eval操作只在rank 0上进行，一次大概消耗将近15s, 就意味着rank0与其它进程进度相差15s, 
            #           只需要40次eval操作各卡之间的进度差距就会被拉开到10min，从而超过nccl最大同步时间而超时。
            dist.barrier()
            print(f"after eval barrier, device={device}, step={step}, train_loss={loss.item()}")

            
        # TODO 带着DDP保存和不带有什么区别？
        if save_strategy == "step" and global_steps % save_steps == 0:
            if is_main_process:
                checkpoint_path = f"{output_dir}/checkpoint-{global_steps}.pth"
                # 保存下一轮要训练的step
                save_model(model, optimizer, checkpoint_path, cur_epoch, step+1)
                # 这里保存checkpoint只耗时9s，但nccl timeout是10分钟，这是如何超时的？
                print(f"{cur_time()} {device}-save checkpoint: {checkpoint_path}")
            
            # 所有进程需要等待rank 0保存完checkpoint
            dist.barrier()
            print(f"{device} after save barrier.")

    return losses

def init_distributed_mode():
    rank = int(os.environ.get("RANK", -1))
    
    ddp = False if rank == -1 else True
    if ddp == False: return False, -1, 'cpu'

    # os.environ['TORCH_NCCL_BLOCKING_WAIT'] = '1'
    # os.environ['TORCH_NCCL_ASYNC_ERROR_HANDLING'] = '1'
    os.environ['NCCL_DEBUG'] = 'WARN'
    # os.environ['NCCL_DEBUG_SUBSYS'] = 'ALL'

    world_size = int(os.environ["WORLD_SIZE"])
    # 这里的timeout没什么用，只是迟延了timeout中断时间而已，最主要还是在于为什么会引发watchdog timeout
    # 下次尝试异步保存checkpoint
    # 多进程通讯，都去连0号进程，并协商一个彼此的进程号rank
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
    local_rank = int(os.environ['LOCAL_RANK'])
    device = f'cuda:{local_rank}'
    torch.cuda.set_device(device)

    print(f"rank:{rank}, world_size:{world_size}, local_rank: {local_rank}")
    return ddp, local_rank, device

def cleanup():
    dist.destroy_process_group()
    print("clean multi process.")

def save_model(model, optimizer, checkpoint_path, epoch, step):
    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
    local_model = model.module if isinstance(model, DistributedDataParallel) else model
    torch.save({
        "model_state": local_model.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "epoch": epoch,
        "step": step,
    }, checkpoint_path)

def load_from_checkpoint(model, optimizer, checkpoint_path, device='cuda'):
    # 在分布式训练的多GPU环境中，map_location可以确保模型的参数和优化器的状态被加载到正确的GPU上，避免出现设备不匹配而报错。
    local_model = model.module if isinstance(model, DistributedDataParallel) else model
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    local_model.load_state_dict(checkpoint['model_state'])
    optimizer.load_state_dict(checkpoint['optimizer_state'])
    return checkpoint.get('epoch', 0), checkpoint.get('step', 0)

def split_dataset(data, train_ratio, eval_ratio):
    train_len = int(len(data) * train_ratio)
    eval_len = int(len(data) * eval_ratio)
    test_len = len(data) - train_len - eval_len
    return random_split(data, [train_len, eval_len, test_len])

def create_dataloaders(dataset_path, tokenizer, batch_size, device, local_rank=-1, max_lines=0, max_eval_data=1000):
    pad_token_id = tokenizer.unk_token_id
    batch_collator = partial(
        collate,
        pad_token_id = pad_token_id,
        tokenizer = tokenizer, 
        device = device,
    )

    ds = JSONLDataset(dataset_path, tokenizer, max_lines=max_lines)
    train_set, eval_set, test_set = split_dataset(ds, 0.8, 0.1)
    sampler = DistributedSampler(train_set, rank=local_rank) if local_rank >= 0 else None
    
    # 用多进程加载，并不能提高速度
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True, collate_fn=batch_collator, sampler=sampler)
    eval_loader = DataLoader(eval_set[:max_eval_data], batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False, collate_fn=batch_collator)
    test_loader = DataLoader(test_set[:max_eval_data], batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False, collate_fn=batch_collator)
    return train_loader, eval_loader, test_loader



def main():
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    epochs = 10
    max_tokens = 1024
    learning_rate = 1e-4
    batch_size = 8

    
    last_checkpoint_path = "/data2/minigpt/models/sft/20241020/checkpoint-60000.pth"
    dataset_path = "/data2/minigpt/dataset/sft/sft_data_zh.jsonl"
    output_dir = "/data2/minigpt/models/sft/20241020"
    tokenizer_path = "/data2/minigpt/models/tokenizer_v3"


    # 进程分布式
    device_type = 'cuda' if torch.cuda.is_available() else 'cpu'
    ddp, local_rank, device = init_distributed_mode()
    print(f"{cur_time()} ddp: {ddp}, local_rank: {local_rank}, device: {device}")

    start_time = time.time()
    start_epoch, start_step = 0, 0
    torch.manual_seed(123)
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)
    
    # 数据加载要设置分布式采样器 TODO 测试num_workers的效果，看是否能加速加载过程， persistent_workers=True用于epoch结束后不关闭workers
    train_loader, eval_loader, _ = create_dataloaders(dataset_path, tokenizer, batch_size, device, local_rank)
    print(f"{cur_time()} create train_loader: {len(train_loader)}, eval_loader: {len(eval_loader)}, use time: {time.time()-start_time:.2f}s")

    # 模型分布式, autocast会自动将float32绽放为float16（autocast不支持bfloat16），这里不用指定数据类型
    model = GPTModel(MODEL_CONFIG).to(device)  # 每个进程将模型放到自己的卡上
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scaler = torch.amp.GradScaler(device_type, enabled=True)

    if last_checkpoint_path != "":
        # 各自加载自己的checkpoint TODO rank=0加载后同步到其它进程
        start_epoch, start_step = load_from_checkpoint(model, optimizer, last_checkpoint_path, device)
        print(f"{cur_time()} load checkpoint[{start_epoch}，{start_step}]: {last_checkpoint_path}")

    # 告诉tensorboard，要将日志记录到哪个文件夹
    writer = SummaryWriter(f"{output_dir}/log")
    # 取一个样例数据，连同model一起传给add_graph函数，它将能够从这个样例数据的预测过程中，收集到模型的计算图
    x_sample, _, _ = next(iter(train_loader))
    writer.add_graph(model, input_to_model=x_sample)

    if ddp:
        # 位置编码用的是复数，而nccl不支持复数形式，此变量并不要求在多进程中保持一致，所以暂时屏蔽对此变量的同步，
        model._ddp_params_and_buffers_to_ignore = {"pos_cis"}
        model = DistributedDataParallel(model, device_ids=[local_rank])
        print(f"{cur_time()} {local_rank},packaged model with DDP in device: {device}")

    train_args = {
        "train_batch_size": batch_size,
        "eval_strategy": "step",
        "eval_steps": 1000,
        "save_strategy": "step",
        "save_steps": 30000,
        "num_train_epochs": epochs,
    }

    for epoch in range(start_epoch, epochs):
        train_loader.sampler.set_epoch(epoch) # 每个epoch开始时都重新打乱数据
        step = start_step if start_epoch == epoch else 0
        model.train()
        train_epoch(epoch, step, train_loader, eval_loader, model, optimizer, scaler, output_dir, device, writer, train_args)
        if not ddp or dist.get_rank() == 0 and train_args.get("save_strategy") == "epoch":
            checkpoint_path = f"{output_dir}/checkpoint-{epoch}.pth"
            save_model(model, optimizer, checkpoint_path, epoch+1, 0)
            print(f"{local_rank}-save checkpoint: {checkpoint_path}")

    writer.close()
    cleanup()


def generate():
    device='cuda:4'
    model = GPTModel(MODEL_CONFIG).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    checkpoint_path = "/data2/minigpt/models/sft/20241020/checkpoint-330000.pth"
    load_from_checkpoint(model, optimizer, checkpoint_path, device)
    
    tokenizer_path = "/data2/minigpt/models/tokenizer_v3"
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)

    messages = [{"role": "user", "content": "如果我需要一个适合智能家居的中央处理器，你有什么推荐吗？"}]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = torch.tensor([tokenizer.encode(prompt)]).to(device)
    response_ids = model.generate(inputs, 512, tokenizer.eos_token_id)
    print(tokenizer.decode(response_ids.squeeze(0), skip_special_tokens=False))


# torchrun --nproc_per_node 2 1-pretrain.py
# I/O
if __name__ == "__main__":
    generate()
    # main()

            