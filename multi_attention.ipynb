{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.引言\n",
    "\n",
    "多头注意力是指将注意力机制分为多个“头”，每个头学习数据的不同方面，使模型能够从多个层面来捕获输入序列中各部分之间的关系，这提高了模型在复杂任务中的表现。\n",
    "\n",
    "本文将在上篇文章[因果注意力]()的基础上，一步一步解读多头注意力的简单实现方法和高效实现方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.堆叠实现\n",
    "\n",
    "多头注意力最直接的实现方法是将多个单头注意力的实例堆叠在一起，如下图所示:\n",
    "![多头堆叠](./img/7-1.jpg)\n",
    "\n",
    " > 图中有两个头的多头注意力模块，每个头都有自己的权重矩阵Wq/Wk/Wv，都有自己中间状态Q/K/V，最后合并两组上下文向量Z1/Z2得到上下文向量矩阵Z。\n",
    "\n",
    " 我们通过将前面已经实现的CausalAttention进行堆叠来演示。首先运行已经封装好的`attention_v1.py`（前文的因果注意力已经封装在里面）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run attention_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个多头注意力类MultiHeadAttentionStack，构造函数与CausalAttention的唯一区别是多了一个num_heads入参，用来指定多头的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttentionStack(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, context_length, dropout_rate, num_heads):\n",
    "        super().__init__()\n",
    "        multi_attens = [CausalAttention(dim_in, dim_out, context_length, dropout_rate) for i in range(num_heads)]\n",
    "        self.heads = nn.ModuleList(multi_attens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vecs = [head(x) for head in self.heads]\n",
    "        return torch.cat(context_vecs, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用刚封装的多头注意力来计算上下文向量，输出维度设为了2，但由于num_heads=2指定了使用两个头，所以最终的上下文向量是dim_out * num_heads = 4 维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1855, 0.8812, 1.3211, 0.8098],\n",
       "         [0.3116, 0.9549, 1.6063, 1.1493],\n",
       "         [0.3395, 0.9652, 1.6530, 1.2084],\n",
       "         [0.3129, 0.8747, 1.5012, 1.0955],\n",
       "         [0.2865, 0.7897, 1.4100, 1.0398],\n",
       "         [0.2990, 0.8040, 1.4025, 1.0361]],\n",
       "\n",
       "        [[0.1855, 0.8812, 1.3211, 0.8098],\n",
       "         [0.3116, 0.9549, 1.6063, 1.1493],\n",
       "         [0.3395, 0.9652, 1.6530, 1.2084],\n",
       "         [0.3129, 0.8747, 1.5012, 1.0955],\n",
       "         [0.2865, 0.7897, 1.4100, 1.0398],\n",
       "         [0.2990, 0.8040, 1.4025, 1.0361]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "b, token_len, dim = batch.shape \n",
    "multi_heads = MultiHeadAttentionStack(dim, 2, token_len, 0.0, num_heads=2)\n",
    "context_vecs = multi_heads(batch)\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 批量输入张量batch中有两个输入文本，由于两个文本是经过复制而来，因此最终的两个文本的上下文张量完全相同。\n",
    "\n",
    "这样，我们就通过创建并组合多个CausalAttention类，实现了第一个版本的多头注意力模块。但这个实现方式有两个缺点：\n",
    "1. 需要关心并维护MultiHeadAttentionStack和CausalAttention两个概念，不够简洁。\n",
    "2. 多个头在计算注意力时需要按顺序以串行的方式，相当于有多少个头就要计算多少遍注意力，不够高效。\n",
    "\n",
    "为此，我们接下来将尝试改进这个实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 权重分割实现\n",
    "\n",
    "**权重分割的基本思想**：将大权重矩阵按照头的数量（num_heads）分割，即能模拟出多头各有一个小的权重矩阵的效果。通过这些小的权重矩阵并行的对输入进行变换，就能一次性计算出所有注意力头的输出。\n",
    "![权重分割](./img/7-2.png)\n",
    "\n",
    "> 如上图所示，通过初始化一个大的权重矩阵`Wq`，只和输入张量`X`进行一次矩阵乘法来获得查询矩阵`Q`，然后使用view方法对`Q`进行形状重塑即可得到多个头。\n",
    "\n",
    "下面我们来进行代码实现。\n",
    "\n",
    "#### 3.1 多头注意力类\n",
    "\n",
    "由于多头注意力的代码相对稍长，我们分成构造方法和forward方法两部分来说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, context_length, dropout_rate, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim_out % num_heads == 0, \"dim_out must be divisible by num_heads\"\n",
    "        self.dim_out = dim_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim_out // num_heads   # 每个头的维度\n",
    "        self.Wq = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.Wk = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.Wv = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.Wo = nn.Linear(dim_out, dim_out)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造方法`__init__`与CausalAttention中的实现基本类似，只作了两个小的扩展：\n",
    "1. 进行了单头维数head_dim的计算，num_heads是多头数量，dim_out是总的输出维度，dim_out/num_heads就是单个头的维度。\n",
    "2. 增加了一个输出投影层（self.Wo），这个并不是必需，但在许多大语言模型中都常见，所以这里也加进来。\n",
    "\n",
    "下面对forward方法进行封装，由于需要对矩阵进行分割和重组，所以会涉及到张量的变形（view）和转置（transpose）操作，需要仔细理解每一步运算过后矩阵形状的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    # 输入形状\n",
    "    b, num_tokens, dim_in = x.shape\n",
    "    # 求Q\\K\\V矩阵，形状变为： b, num_tokens, dim_out\n",
    "    q = self.Wq(x)\n",
    "    k = self.Wk(x)\n",
    "    v = self.Wv(x)\n",
    "\n",
    "    # 变换形状，将最后一维拆成多头，每个头有head_dim维，矩阵形状由三维变为四维。\n",
    "    q = q.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    k = k.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    v = v.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    \n",
    "    # 交换第2维和第3维，这一步过后，形状变为：b, num_heads, num_tokens, head_dim\n",
    "    q = q.transpose(1, 2)   \n",
    "    k = k.transpose(1, 2)\n",
    "    v = v.transpose(1, 2)\n",
    "\n",
    "    # 计算注意力分数，形状变为: b, num_heads, num_tokens, num_tokens\n",
    "    atten_scores = q @ k.transpose(2, 3)   \n",
    "    atten_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "    atten_weights = torch.softmax(atten_scores/k.shape[-1]**0.5, dim=-1)\n",
    "    atten_weights = self.dropout(atten_weights)\n",
    "\n",
    "    # 计算上下文张量\n",
    "    context_vecs = atten_weights @ v   # shape: b, num_heads, num_tokens, head_dim\n",
    "    context_vecs = context_vecs.transpose(1,2)  # shape: b, num_tokens, num_heads, head_dim\n",
    "    context_vecs = context_vecs.contiguous().view(b, num_tokens, self.dim_out)\n",
    "    output = self.Wo(context_vecs)\n",
    "\n",
    "    return output\n",
    "\n",
    "setattr(MultiHeadAttention, \"forward\", forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几个关注步骤的理解:\n",
    "1. 关键操作是将dim_out维度分割为num_heads和head_dim，这个分割操作由view方法实现，将维度为(b, num_tokens, dim_out)的张量q、k、v重塑为维度(b, num_tokens, num_heads, head_dim),即将一个大的head切割成了多头。\n",
    "2. 随后进行了张量转置操作，将多头维度num_heads排在了序列长度num_tokens之前，张量形状变为（b, num_heads, num_tokens, head_dim），正是由于这个转置，使得多个头可以一次性进行批量矩阵乘法，一次性得出所有上下文向量。\n",
    "3. 最后将两个维度num_heads和num_tokens恢复位置，形状变回(b, num_tokens, num_heads, head_dim)，再通过view操作将多个头的上下文向量合并，向量被重塑（扁平化）成 (b, num_tokens, dim_out) 的形状。\n",
    "\n",
    "> contiguous方法：张量在某些复杂操作（例如转置、切片）之后可能会变得不连续，而view方法需要连续的内存才能正常工作，contiguous方法的作用就是确保返回一个在内存中连续的张量。\n",
    "\n",
    "#### 3.2 批量矩阵乘法\n",
    "上面的q\\k\\v矩阵在进行权重分割后变成了四维张量，在pytorch中进行四维张量乘法时，矩阵乘法运算将会在最后两个维度之间进行，等价于各个头内部分别运算，最后再堆叠到一起。运算过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.1855, 0.8812, 1.3211, 0.8098],\n",
    "                 [0.3116, 0.9549, 1.6063, 1.1493],\n",
    "                 [0.3395, 0.9652, 1.6530, 1.2084]],\n",
    "                  \n",
    "                 [[0.3129, 0.8747, 1.5012, 1.0955],\n",
    "                 [0.2865, 0.7897, 1.4100, 1.0398],\n",
    "                 [0.2990, 0.8040, 1.4025, 1.0361]]]])\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们可以把上面这个矩阵看作是拥有两个头、每个头有3行4列的多头张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_head: tensor([[0.1855, 0.8812, 1.3211, 0.8098],\n",
      "        [0.3116, 0.9549, 1.6063, 1.1493],\n",
      "        [0.3395, 0.9652, 1.6530, 1.2084]])\n",
      "second_head: tensor([[0.3129, 0.8747, 1.5012, 1.0955],\n",
      "        [0.2865, 0.7897, 1.4100, 1.0398],\n",
      "        [0.2990, 0.8040, 1.4025, 1.0361]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0,0,:,:]\n",
    "second_head = a[0,1,:,:]\n",
    "print(\"first_head:\", first_head)\n",
    "print(\"second_head:\", second_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "分别对每个头进行矩阵转置后的乘法运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_head_res:  tensor([[3.2120, 3.9520, 4.0759],\n",
      "        [3.9520, 4.9100, 5.0715],\n",
      "        [4.0759, 5.0715, 5.2395]])\n",
      "second_head_res: tensor([[4.3167, 4.0362, 4.0373],\n",
      "        [4.0362, 3.7750, 3.7754],\n",
      "        [4.0373, 3.7754, 3.7763]])\n"
     ]
    }
   ],
   "source": [
    "first_head_res = first_head @ first_head.T\n",
    "second_head_res = second_head @ second_head.T\n",
    "print(\"first_head_res: \", first_head_res)\n",
    "print(\"second_head_res:\", second_head_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的过程可以视为一个批量矩阵乘法运算，等于是将张量`a`和转置了最后两个维度的张量a的视图之间进行批量矩阵乘法，整个过程可以简单用一句代码表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3.2120, 3.9520, 4.0759],\n",
       "          [3.9520, 4.9100, 5.0715],\n",
       "          [4.0759, 5.0715, 5.2395]],\n",
       "\n",
       "         [[4.3167, 4.0362, 4.0373],\n",
       "          [4.0362, 3.7750, 3.7754],\n",
       "          [4.0373, 3.7754, 3.7763]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ a.transpose(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，这个结果和上面多个头单独运算的结果完全相同。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 多头注意力使用\n",
    "下面用之前两个序列的输入来示例MultiHeadAttention的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
       "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
       "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
       "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
       "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
       "         [-0.0132,  0.2990, -0.0689, -0.3490]],\n",
       "\n",
       "        [[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
       "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
       "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
       "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
       "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
       "         [-0.0132,  0.2990, -0.0689, -0.3490]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "b, num_tokens, dim_in = batch.shape\n",
    "multi_heads = MultiHeadAttention(dim_in, 4, num_tokens, 0.0, 2)\n",
    "context_vecs = multi_heads(batch)\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 虽然 MultiHeadAttention 类看起来比 MultiHeadAttentionStack 更复杂，但它更高效。原因在于我们每一步都只需要一次矩阵乘法，例如`k = self.Wk(x)`执行一遍就可以计算出所有键，这对查询、值、注意力权重等都适用。与之相对的是，在 MultiHeadAttentionStack 中，我们需要按头的数量重复这一矩阵乘法，这在计算上是最昂贵的步骤之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小结**：本节中我们首先通过堆叠多个因果注意力演示了什么是多头，然后通过权重分割和形状变换的方式实现了一个高效的MultiHeadAttention类，并对多维下批量矩阵乘法的运算规则作了说明，这个多头注意力类后面将会用于训练大语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
