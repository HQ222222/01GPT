{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引言\n",
    "\n",
    "分词器是每个大语言模型必不可少的组件，但每个大语言模型的分词器几乎都不相同。如果要训练自己的分词器，可以使用huggingface的tokenizers框架，tokenizers包含以下主要组件：\n",
    "1. Tokenizer: 分词器的核心组件，定义了分词的整个流程，包括标准化、预分词、模型分词、后处理等\n",
    "2. Normalizers：可选，负责将文本标准化，包括unicode归一化、大写转小写、去重音等操作\n",
    "3. Pre-tokenizers：负责将文本分割成更小的片段（如单词等），为模型分词做准备。常见的预分词器有按空格分词（Whitespace）、正则表达式分词（Regex）等\n",
    "4. Models：是实际的分词算法，负责将文本片段转换为子词，常见的有BPE、WordPiece、Unigram等。\n",
    "5. Post-Processors：负责对分词结果进行后处理，如添加特殊标记（CLS、SEP）。\n",
    "6. Decoders：负责将分词结果转换回原始文本，常见的解码器有 ByteLevel、WordPiece 等。\n",
    "7. Trainers：用于训练分词模型，不同的模型对应不同的训练器，如 BpeTrainer、WordPieceTrainer、UnigramTrainer 等。\n",
    "\n",
    "在开始之前，先导入对应的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载语料库\n",
    "\n",
    "我们准备好的语料库是一个jsonl文件，大概有736MB，每一行是一条json格式的文本数据，既有中文，也有英文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua 736729803 Nov  4 22:04 /data2/minigpt/dataset/tokenize/tokenizer_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -l /data2/minigpt/dataset/tokenize/tokenizer_train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个函数用于从JSONL文件中读取文本数据，考虑到语料库会比较大，所以采用`yield`生成器来延迟到访问时再加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            data = json.loads(line)\n",
    "            yield data['text']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/data2/minigpt/dataset/tokenize/tokenizer_train.jsonl'\n",
    "texts = read_texts_from_jsonl(data_path)\n",
    "type(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，函数`read_texts_from_jsonl`返回的并不是真正的数据，而是一个生成器`generator`。可以通过`next`函数像访问`iterator`一样访问迭代数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'好的。现在请你将这个文本中的所有的逗号都替换成空格。 好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程\n",
    "\n",
    "#### 模型选择\n",
    "使用BPE模型来初始化Tokenizer实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE是一种基于子词的分词方法，例如：\n",
    "- cats -> cat + s\n",
    "- helpful -> help + ful\n",
    "- congratulation -> con + gr + at + ulation\n",
    "\n",
    "这种基于子词的分词方法，相比基于完整单词和基于单个字符有以下好处：\n",
    "1. 子词相比于单词（可以认为多个子词的组合）数量要可控，这能避免词表过大，并且能避免生僻词带来的未知令牌问题。\n",
    "2. 子词相比于字符语义性更强，像单个字符`f`是没有语义的，但子词`ful`可以表达`满的`，比较像英语里的词根词缀。\n",
    "\n",
    "#### 预分词器选择\n",
    "为tokenizer设置预分词器，预分词器有以下几类：\n",
    "- Whitespace：按空格分隔，粒度为单词，适用于空格分隔的语言，例如英语。\n",
    "- Regex：按自定义正则表达式分隔，适用于需要自定义复杂分词规则的场景。\n",
    "- ByteLevel：按字节分割，适用于特殊字符、非英语场景（例如中文）。\n",
    "\n",
    "由于我们主要面向中文，所以这里采用ByteLevel的pre_tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.pre_tokenizers.ByteLevel at 0x7f41641266f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'add_prefix_space',\n",
       " 'alphabet',\n",
       " 'custom',\n",
       " 'pre_tokenize',\n",
       " 'pre_tokenize_str',\n",
       " 'use_regex']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer.pre_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那pre_tokenizer具体对文本作了什么处理呢？可以通过下面几个例子来观察下。\n",
    "1. 处理英文文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pre', (0, 3)),\n",
       " ('-', (3, 4)),\n",
       " ('tokenize', (4, 12)),\n",
       " ('Ġa', (12, 14)),\n",
       " ('Ġ:', (14, 16)),\n",
       " ('class', (16, 21)),\n",
       " (':`~', (21, 24)),\n",
       " ('tokenizers', (24, 34)),\n",
       " ('.', (34, 35)),\n",
       " ('PyPreTokenizedString', (35, 55)),\n",
       " ('`', (55, 56)),\n",
       " ('Ġin', (56, 59)),\n",
       " ('-', (59, 60)),\n",
       " ('place', (60, 65))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Pre-tokenize a :class:`~tokenizers.PyPreTokenizedString` in-place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，pre_tokenizer将文本按照空格和特殊字符作了初步分词，空格处理成了特殊字符`Ġ`，并记录了每个词的起始和结束位置。\n",
    "\n",
    "2. 处理中文文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('åľ¨æŁ¥å¤ĦèĻļå¼Ģå¢ŀåĢ¼ç¨İä¸ĵçĶ¨åıĳç¥¨æ¡Īä»¶ä¸Ń', (0, 15)),\n",
       " ('ï¼Į', (15, 16)),\n",
       " ('å¸¸å¸¸æ¶īåıĬè¿Ľé¡¹çķĻæĬµç¨İé¢ĿåĴĮç¨İæ¬¾æįŁå¤±çļĦè®¤å®ļåĴĮå¤ĦçĲĨ', (16, 37)),\n",
       " ('ãĢĤ', (37, 38))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_sentence = \"在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。\"\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(zh_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文基本也是按照特殊符号`，`和`。`进行了分词，但分词的结果是一堆不认识的字符，这些字符是如何产生的呢？\n",
    "\n",
    "#### 预分词原理探究\n",
    "\n",
    "预分词常常使用类似下面一样的正则表达式先对文本进行分隔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在查处虚开增值税专用发票案件中', '，', '常常涉及进项留抵税额和税款损失的认定和处理', '。']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "PRETOKENIZE_REGEX = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}\\p{P}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "pat = re.compile(PRETOKENIZE_REGEX)\n",
    "tokens = re.findall(pat, zh_sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，各部分正则表达式的作用如下：\n",
    "-  (?i:'s|'t|'re|'ve|'m|'ll|'d): 匹配常见的英文缩略形式，例如：'s（is 或 has），'t（not），'re（are），'ve（have），'m（am），'ll（will），'d（would 或 had）。\n",
    "-  [^\\r\\n\\p{L}\\p{N}\\p{P}]?\\p{L}+：匹配一个或多个 Unicode 字母`\\p{L}`，这里的unicode字母包括英文、中文、拉丁等所有语言中的字母，允许前面有一个非换行符`\\r\\n`、非字母`\\p{L}`、非数字`\\p{N}`和非标点`\\p{P}`的字符，相当于是匹配空格、制表符等空白字符。\n",
    "- \\p{N}：匹配任何 Unicode 数字字符。\n",
    "- ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*：匹配非空白、非字母、非数字的字符，允许前面有一个空格，后面跟随换行符，相当于是匹配标点符号后面跟换行符。\n",
    "- \\p{L}：匹配任何 Unicode 字母字符，包括拉丁字母、希腊字母、汉字等所有语言中的字母。\n",
    "- \\p{N}：匹配任何 Unicode 数字字符，涵盖阿拉伯数字、罗马数字等所有形式的数字字符。\n",
    "- \\p{P}：匹配任何 Unicode 标点字符，涵盖句号、逗号、引号、括号等所有形式、所有语言中的标点符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于英文以外的其它语言（例如中文），需要进行utf-8编码，将字符编码为字节，目的是解决英文、中文、日文、俄文等多语言的问题，因为世界上所有语言的字符都可以用一个或多个utf-8字节的组合来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'\\xe5\\x9c\\xa8\\xe6\\x9f\\xa5\\xe5\\xa4\\x84\\xe8\\x99\\x9a\\xe5\\xbc\\x80\\xe5\\xa2\\x9e\\xe5\\x80\\xbc\\xe7\\xa8\\x8e\\xe4\\xb8\\x93\\xe7\\x94\\xa8\\xe5\\x8f\\x91\\xe7\\xa5\\xa8\\xe6\\xa1\\x88\\xe4\\xbb\\xb6\\xe4\\xb8\\xad',\n",
       " b'\\xef\\xbc\\x8c',\n",
       " b'\\xe5\\xb8\\xb8\\xe5\\xb8\\xb8\\xe6\\xb6\\x89\\xe5\\x8f\\x8a\\xe8\\xbf\\x9b\\xe9\\xa1\\xb9\\xe7\\x95\\x99\\xe6\\x8a\\xb5\\xe7\\xa8\\x8e\\xe9\\xa2\\x9d\\xe5\\x92\\x8c\\xe7\\xa8\\x8e\\xe6\\xac\\xbe\\xe6\\x8d\\x9f\\xe5\\xa4\\xb1\\xe7\\x9a\\x84\\xe8\\xae\\xa4\\xe5\\xae\\x9a\\xe5\\x92\\x8c\\xe5\\xa4\\x84\\xe7\\x90\\x86',\n",
       " b'\\xe3\\x80\\x82']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_utf8 = [token.encode(\"utf-8\") for token in tokens]\n",
    "tokens_utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但有个问题是：Ascii码中是会包含回车、制表、换行等控制字符的，同样utf-8编码中也会有。而我们最终构造的词表必须是可显示的文本，所以还要做一个工作是把控制字符都转换为可显示字符，为此需要制作一个unicode字节编码表，用于将单字节（256以内）都编码为可显示字符。\n",
    "\n",
    "0-255范围内的可显示字符分为三段：\n",
    "- 从 !（ASCII 33）到 ~（ASCII 126）。\n",
    "- 从 ¡（Unicode 161）到 ¬（Unicode 172）。\n",
    "- 从 ®（Unicode 174）到 ÿ（Unicode 255）。\n",
    "\n",
    "这三段以外的ASCII码均无法正常显示，需要用可显示字符来填充替代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"33\": \"!\", \"34\": \"\\\\\"\", \"35\": \"#\", \"36\": \"$\", \"37\": \"%\", \"38\": \"&\", \"39\": \"\\'\", \"40\": \"(\", \"41\": \")\", \"42\": \"*\", \"43\": \"+\", \"44\": \",\", \"45\": \"-\", \"46\": \".\", \"47\": \"/\", \"48\": \"0\", \"49\": \"1\", \"50\": \"2\", \"51\": \"3\", \"52\": \"4\", \"53\": \"5\", \"54\": \"6\", \"55\": \"7\", \"56\": \"8\", \"57\": \"9\", \"58\": \":\", \"59\": \";\", \"60\": \"<\", \"61\": \"=\", \"62\": \">\", \"63\": \"?\", \"64\": \"@\", \"65\": \"A\", \"66\": \"B\", \"67\": \"C\", \"68\": \"D\", \"69\": \"E\", \"70\": \"F\", \"71\": \"G\", \"72\": \"H\", \"73\": \"I\", \"74\": \"J\", \"75\": \"K\", \"76\": \"L\", \"77\": \"M\", \"78\": \"N\", \"79\": \"O\", \"80\": \"P\", \"81\": \"Q\", \"82\": \"R\", \"83\": \"S\", \"84\": \"T\", \"85\": \"U\", \"86\": \"V\", \"87\": \"W\", \"88\": \"X\", \"89\": \"Y\", \"90\": \"Z\", \"91\": \"[\", \"92\": \"\\\\\\\\\", \"93\": \"]\", \"94\": \"^\", \"95\": \"_\", \"96\": \"`\", \"97\": \"a\", \"98\": \"b\", \"99\": \"c\", \"100\": \"d\", \"101\": \"e\", \"102\": \"f\", \"103\": \"g\", \"104\": \"h\", \"105\": \"i\", \"106\": \"j\", \"107\": \"k\", \"108\": \"l\", \"109\": \"m\", \"110\": \"n\", \"111\": \"o\", \"112\": \"p\", \"113\": \"q\", \"114\": \"r\", \"115\": \"s\", \"116\": \"t\", \"117\": \"u\", \"118\": \"v\", \"119\": \"w\", \"120\": \"x\", \"121\": \"y\", \"122\": \"z\", \"123\": \"{\", \"124\": \"|\", \"125\": \"}\", \"126\": \"~\", \"161\": \"¡\", \"162\": \"¢\", \"163\": \"£\", \"164\": \"¤\", \"165\": \"¥\", \"166\": \"¦\", \"167\": \"§\", \"168\": \"¨\", \"169\": \"©\", \"170\": \"ª\", \"171\": \"«\", \"172\": \"¬\", \"174\": \"®\", \"175\": \"¯\", \"176\": \"°\", \"177\": \"±\", \"178\": \"²\", \"179\": \"³\", \"180\": \"´\", \"181\": \"µ\", \"182\": \"¶\", \"183\": \"·\", \"184\": \"¸\", \"185\": \"¹\", \"186\": \"º\", \"187\": \"»\", \"188\": \"¼\", \"189\": \"½\", \"190\": \"¾\", \"191\": \"¿\", \"192\": \"À\", \"193\": \"Á\", \"194\": \"Â\", \"195\": \"Ã\", \"196\": \"Ä\", \"197\": \"Å\", \"198\": \"Æ\", \"199\": \"Ç\", \"200\": \"È\", \"201\": \"É\", \"202\": \"Ê\", \"203\": \"Ë\", \"204\": \"Ì\", \"205\": \"Í\", \"206\": \"Î\", \"207\": \"Ï\", \"208\": \"Ð\", \"209\": \"Ñ\", \"210\": \"Ò\", \"211\": \"Ó\", \"212\": \"Ô\", \"213\": \"Õ\", \"214\": \"Ö\", \"215\": \"×\", \"216\": \"Ø\", \"217\": \"Ù\", \"218\": \"Ú\", \"219\": \"Û\", \"220\": \"Ü\", \"221\": \"Ý\", \"222\": \"Þ\", \"223\": \"ß\", \"224\": \"à\", \"225\": \"á\", \"226\": \"â\", \"227\": \"ã\", \"228\": \"ä\", \"229\": \"å\", \"230\": \"æ\", \"231\": \"ç\", \"232\": \"è\", \"233\": \"é\", \"234\": \"ê\", \"235\": \"ë\", \"236\": \"ì\", \"237\": \"í\", \"238\": \"î\", \"239\": \"ï\", \"240\": \"ð\", \"241\": \"ñ\", \"242\": \"ò\", \"243\": \"ó\", \"244\": \"ô\", \"245\": \"õ\", \"246\": \"ö\", \"247\": \"÷\", \"248\": \"ø\", \"249\": \"ù\", \"250\": \"ú\", \"251\": \"û\", \"252\": \"ü\", \"253\": \"ý\", \"254\": \"þ\", \"255\": \"ÿ\", \"0\": \"Ā\", \"1\": \"ā\", \"2\": \"Ă\", \"3\": \"ă\", \"4\": \"Ą\", \"5\": \"ą\", \"6\": \"Ć\", \"7\": \"ć\", \"8\": \"Ĉ\", \"9\": \"ĉ\", \"10\": \"Ċ\", \"11\": \"ċ\", \"12\": \"Č\", \"13\": \"č\", \"14\": \"Ď\", \"15\": \"ď\", \"16\": \"Đ\", \"17\": \"đ\", \"18\": \"Ē\", \"19\": \"ē\", \"20\": \"Ĕ\", \"21\": \"ĕ\", \"22\": \"Ė\", \"23\": \"ė\", \"24\": \"Ę\", \"25\": \"ę\", \"26\": \"Ě\", \"27\": \"ě\", \"28\": \"Ĝ\", \"29\": \"ĝ\", \"30\": \"Ğ\", \"31\": \"ğ\", \"32\": \"Ġ\", \"127\": \"ġ\", \"128\": \"Ģ\", \"129\": \"ģ\", \"130\": \"Ĥ\", \"131\": \"ĥ\", \"132\": \"Ħ\", \"133\": \"ħ\", \"134\": \"Ĩ\", \"135\": \"ĩ\", \"136\": \"Ī\", \"137\": \"ī\", \"138\": \"Ĭ\", \"139\": \"ĭ\", \"140\": \"Į\", \"141\": \"į\", \"142\": \"İ\", \"143\": \"ı\", \"144\": \"Ĳ\", \"145\": \"ĳ\", \"146\": \"Ĵ\", \"147\": \"ĵ\", \"148\": \"Ķ\", \"149\": \"ķ\", \"150\": \"ĸ\", \"151\": \"Ĺ\", \"152\": \"ĺ\", \"153\": \"Ļ\", \"154\": \"ļ\", \"155\": \"Ľ\", \"156\": \"ľ\", \"157\": \"Ŀ\", \"158\": \"ŀ\", \"159\": \"Ł\", \"160\": \"ł\", \"173\": \"Ń\"}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bytes_to_unicode():\n",
    "    # 收集0-255范围内的可显示字符对应的数字值，ord函数用于将字符编码为数字\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1)) + \n",
    "        list(range(ord(\"¡\"), ord(\"¬\") + 1)) + \n",
    "        list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    # 补充0-255范围内不可显示字符对应的数字，并转换为256以上可显示字符对应的数字值\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    # chr函数用于将数字转换回unicode字符，并创建一个字节值到字符值的映射表。\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "byte_encoder = bytes_to_unicode()\n",
    "json.dumps(byte_encoder, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 这样，每个字节值都从 Unicode 表的开头获得一个分配给它的“可见”字符。这一点非常重要，因为每个utf-8字符都是由一到多个字节组成的，将这个长度为256的编码表中的字节进行组合，理论上就能对世界上所有语言中的字符进行编码，并且还不会出现`未知`标记。\n",
    "\n",
    "使用这个unicode字节编码表将前面utf-8编码后的文本序列进行ByteLevel级的编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['åľ¨æŁ¥å¤ĦèĻļå¼Ģå¢ŀåĢ¼ç¨İä¸ĵçĶ¨åıĳç¥¨æ¡Īä»¶ä¸Ń',\n",
       " 'ï¼Į',\n",
       " 'å¸¸å¸¸æ¶īåıĬè¿Ľé¡¹çķĻæĬµç¨İé¢ĿåĴĮç¨İæ¬¾æįŁå¤±çļĦè®¤å®ļåĴĮå¤ĦçĲĨ',\n",
       " 'ãĢĤ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_unicode = [\"\".join(byte_encoder[b] for b in token) for token in tokens_utf8]\n",
    "tokens_unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，结果与使用pre_tokenizer预分词的结果完全相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建训练器\n",
    "BPE训练器中需要指定几个参数：\n",
    "- vocab_size：训练后词表中的词条数量，BPE是一个从短词到长词的组合过程，达到词表大小后就会停止训练。\n",
    "- special_tokens：特殊token，和语言模型的特殊token相同，例如开始、结束、填充标记。\n",
    "- initial_alphabet：初始字符表，使用上面长度为256的unicode字节编码表作为初始字符表。\n",
    "\n",
    "通过`pre_tokenizers.ByteLevel.alphabet()`可以获得初始字符编码表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"ø\", \"\\\\\\\\\", \"ľ\", \"v\", \"ć\", \"¬\", \"ł\", \"°\", \"ġ\", \"ķ\", \"ĕ\", \"»\", \"]\", \"Q\", \"ģ\", \"G\", \"ñ\", \"¶\", \"é\", \"H\", \"9\", \")\", \"×\", \"Í\", \"Ó\", \"º\", \"£\", \"~\", \"Ā\", \"s\", \"Ô\", \"2\", \"Ý\", \"í\", \"â\", \"·\", \"Ą\", \"ý\", \"ĭ\", \"²\", \"4\", \"Ù\", \"ĺ\", \"ă\", \"ĸ\", \"Ī\", \"z\", \"K\", \"Ĳ\", \"N\", \"Ø\", \"1\", \"n\", \"b\", \"ó\", \"¼\", \"õ\", \"V\", \"Ö\", \"6\", \"©\", \"ė\", \"O\", \"đ\", \"j\", \"h\", \"İ\", \"į\", \"¨\", \"¯\", \"Ğ\", \"I\", \"0\", \"Ă\", \"=\", \"ß\", \"Û\", \"Ć\", \"Å\", \"ĩ\", \"Ê\", \"B\", \"ª\", \"W\", \"_\", \"S\", \"Ĥ\", \"Ł\", \"q\", \"ë\", \"Ķ\", \"Ò\", \"Ĉ\", \"Ċ\", \"L\", \"«\", \"U\", \"#\", \"ļ\", \"Ė\", \"å\", \"´\", \"Î\", \"M\", \"&\", \"D\", \"¤\", \"ô\", \"ç\", \"Y\", \"R\", \"ð\", \"Ĺ\", \"ĳ\", \">\", \"Ŀ\", \"ę\", \"d\", \"É\", \"à\", \"ŀ\", \"<\", \"Ĩ\", \"¹\", \"Ã\", \"/\", \"¸\", \"Ģ\", \"X\", \"ê\", \"u\", \"ğ\", \"m\", \"w\", \"Ì\", \"¢\", \"Æ\", \"C\", \"t\", \"Ļ\", \"ì\", \"ě\", \"Ď\", \"l\", \"ē\", \"Ħ\", \"Ñ\", \"3\", \"÷\", \"{\", \"$\", \"y\", \"Ç\", \"¡\", \"Ë\", \"ĉ\", \"ĵ\", \"Ĵ\", \"Č\", \"a\", \"T\", \";\", \"Ń\", \"Ú\", \"f\", \"§\", \"Z\", \"+\", \"\\'\", \"Ä\", \"A\", \"ÿ\", \"Ę\", \"Õ\", \"Ġ\", \"c\", \"%\", \"Ē\", \"ï\", \"ī\", \"Ĕ\", \"?\", \"(\", \"Đ\", \"Ï\", \"ö\", \"^\", \"P\", \"±\", \"x\", \",\", \"i\", \"æ\", \"®\", \"-\", \"³\", \"î\", \"*\", \"û\", \"ú\", \"¾\", \"ä\", \"Į\", \"5\", \"ò\", \"Ĭ\", \"Â\", \"þ\", \"J\", \"ċ\", \":\", \"ü\", \"¥\", \"`\", \"è\", \"È\", \"ĝ\", \"ą\", \"}\", \"!\", \"r\", \"Þ\", \"g\", \"\\\\\"\", \"[\", \"À\", \"ā\", \"ď\", \"7\", \"¿\", \"ħ\", \"F\", \"ı\", \"Á\", \"á\", \"Ð\", \".\", \"@\", \"E\", \"č\", \"½\", \"e\", \"ã\", \"ù\", \"Ľ\", \"Ĝ\", \"ĥ\", \"¦\", \"Ě\", \"p\", \"Ü\", \"8\", \"|\", \"k\", \"o\", \"µ\"]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(pre_tokenizers.ByteLevel.alphabet(), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义特殊token，分别为填充、开始、结束标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建训练器，词条数量设置为32000。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    special_tokens=special_tokens,  # 确保这三个token被包含\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上面的texts生成器作为语料库，使用trainer开始训练分词器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个训练过程的用时长短与文本数据大小有关，我的文本数据大概900多MB, 大概需要十几分钟。\n",
    "\n",
    "#### 保存训练结果\n",
    "\n",
    "在保存结果之前，需要先设置相匹配的解码器，否则ASCII以外的字符可能无法正常解码。\n",
    "\n",
    "> 上面编码阶段使用了ByteLevel的预分词器，相对应的解码阶段也需要使用ByteLevel，表示将token id转换为token后，还需要进行一次unicode字节级别的解码，才能正常显示中文等多语言字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练的分词器保存到指定目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data2/minigpt/models/tokenizer_v3/vocab.json',\n",
       " '/data2/minigpt/models/tokenizer_v3/merges.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dir = \"/data2/minigpt/models/tokenizer_v3\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "tokenizer.model.save(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还需要一个分词器配置文件，包括模型类型、是否使用小写字母等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": True,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<|endoftext|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"content\": \"<|im_start|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"content\": \"<|im_end|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        }\n",
    "    },\n",
    "    \"additional_special_tokens\": [],\n",
    "    \"bos_token\": \"<|im_start|>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"<|im_end|>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 1000000000000000019884624838656,\n",
    "    \"pad_token\": None,\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"use_default_system_prompt\": False,\n",
    "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + content + '<|im_end|>\\\\n<|im_start|>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "保存分词器配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看磁盘上的词表文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2548\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua  407951 Oct 10 21:45 merges.txt\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    1686 Oct 10 21:45 tokenizer_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua 1572840 Oct 10 21:45 tokenizer.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua  621912 Oct 10 21:45 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l /data2/minigpt/models/tokenizer_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab.json：词汇表文件，包含词条和对应的索引。\n",
    "- merges.txt: 合并表文件，定义了子词的合并规则。\n",
    "- tokenizer.json: 完整的分词器文件，它包含了分词器的所有信息，包括词汇表、合并规则、特殊标记等。\n",
    "- tokenizer_config.json: 分词器配置文件，包括了起始token、结束token的定义，以及提示词模板。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'了一些'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer_dir = \"/data2/minigpt/models/tokenizer_v3\"\n",
    "tokenizer_trained = AutoTokenizer.from_pretrained(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "英文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pre',\n",
       " '-',\n",
       " 'token',\n",
       " 'ize',\n",
       " 'Ġa',\n",
       " 'Ġ:',\n",
       " 'class',\n",
       " ':',\n",
       " '`',\n",
       " '~',\n",
       " 'token',\n",
       " 'izers',\n",
       " '.',\n",
       " 'Py',\n",
       " 'Pre',\n",
       " 'T',\n",
       " 'oken',\n",
       " 'ized',\n",
       " 'String',\n",
       " '`',\n",
       " 'Ġin',\n",
       " '-',\n",
       " 'place']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_en = \"Pre-tokenize a :class:`~tokenizers.PyPreTokenizedString` in-place\"\n",
    "tokenized = tokenizer_trained.tokenize(text_en)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize方法只对输入文本作了分词，返回的是一组明文的token。要想直接返回token_id，需要使用encode方法，分词的同时完成文本到数字的序列化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19714,\n",
       " 15,\n",
       " 24535,\n",
       " 1038,\n",
       " 260,\n",
       " 6938,\n",
       " 9939,\n",
       " 28,\n",
       " 66,\n",
       " 96,\n",
       " 24535,\n",
       " 11344,\n",
       " 16,\n",
       " 22966,\n",
       " 19714,\n",
       " 54,\n",
       " 9071,\n",
       " 1228,\n",
       " 13863,\n",
       " 66,\n",
       " 295,\n",
       " 15,\n",
       " 2383]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_en = tokenizer_trained.encode(text_en)\n",
    "token_ids_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pre-tokenize a :class:`~tokenizers.PyPreTokenizedString` in-place'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_trained.decode(token_ids_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "可以看到，解码的结果与原始英文串完全相同。\n",
    "\n",
    "下面测试下中文文本的序列化和反序列化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[368,\n",
       " 1698,\n",
       " 1319,\n",
       " 4304,\n",
       " 953,\n",
       " 30571,\n",
       " 2147,\n",
       " 411,\n",
       " 646,\n",
       " 3917,\n",
       " 6723,\n",
       " 413,\n",
       " 270,\n",
       " 6679,\n",
       " 4743,\n",
       " 631,\n",
       " 1467,\n",
       " 3692,\n",
       " 9083,\n",
       " 3534,\n",
       " 2676,\n",
       " 315,\n",
       " 3534,\n",
       " 1805,\n",
       " 8576,\n",
       " 269,\n",
       " 1374,\n",
       " 627,\n",
       " 12769,\n",
       " 286]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_zh = \"在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。\"\n",
    "token_ids_zh = tokenizer_trained.encode(text_zh)\n",
    "token_ids_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_trained.decode(token_ids_zh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们刚训练的分词器在中文和英文上都能正常进行的文本的序列化和反序列化操作。\n",
    "\n",
    "**小结**：本文借助huggingface提供的tokenizers框架，以一个真实的语料库为案例，演示了分词器训练的过程，并最终得到了一个切实可用的分词器。但tokenizers框架封装的比较多，所以在训练过程中对多语言的编码和解码部分作了内部实现的剖析和讲解，如果你还对其它部分（如BPE算法）感兴趣，下面的参考内容或许能为你提供进一步的帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考阅读\n",
    "- [手搓BPE算法](https://golfxiao.blog.csdn.net/article/details/139106621)\n",
    "- [什么是tokenizer？](https://golfxiao.blog.csdn.net/article/details/138781653)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
