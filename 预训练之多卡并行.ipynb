{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f5ea9e-af30-4bd1-acf1-aa617d43dae9",
   "metadata": {},
   "source": [
    "## 1.引言\n",
    "\n",
    "上一节[预训练模型之运算加速](https://blog.csdn.net/xiaojia1001/article/details/144831811)主要从数据运算层面来优化模型的训练速度，尽管优化效果显著，但始终只能使用单张GPU卡，本节我们将来探讨如何利用多卡来加速模型训练，即分布式训练。\n",
    "\n",
    "那为什么要使用多张卡来训练呢？原因在于当数据量和运算量很大时，会面临以下问题：\n",
    "1. 单块显卡内存限制，无法支撑更大的数据量运算。\n",
    "2. 单块显卡串行计算难以进一步优化，而多块显卡并行运算则能成倍的提升模型训练速度。\n",
    "3. 单块显卡难以使用更大的batch-size（容易OOM），而大的batch-size比小的batch-size训练过程更稳定，效果也更好。\n",
    "\n",
    "在pytorch中，支持通过DDP(DistributedDataParallel)数据并行来实现多卡训练。它的**基本思想**是：将训练数据按照GPU卡的数量划分为多份，并在每个GPU上运行独立的进程来并行训练每份数据，再通过多进程通信来同步梯度和更新参数，从而显著的提高训练速度。\n",
    "\n",
    "因此，我们需要引入必要的跨进程组件，并对训练过程进行改造，才能支持DDP。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2849d7a-cbdc-41ab-a0d3-fef5c8473716",
   "metadata": {},
   "source": [
    "## 2.基础环节改造\n",
    "\n",
    "我们会基于[上一节](https://blog.csdn.net/xiaojia1001/article/details/144831811)构建的训练器Trainer(已经封装到pretrainer_single.py中)进行扩展和改造，以支持并行训练。\n",
    "\n",
    "首先，引入之前已经封装过的模型结构、数据集和预训练器的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98afa572-d7e0-4f21-83bc-5102cbc8b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload  \n",
    "%autoreload 2 \n",
    "%run transformer.py\n",
    "%run pretrain_dataset.py\n",
    "%run pretrainer_single.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872d828-868e-4c37-b3b3-6e119b25a8b2",
   "metadata": {},
   "source": [
    "对DDP多卡并行的改造主要包括几方面，分别是：\n",
    "- 多进程通信\n",
    "- 数据集分片\n",
    "- 模型状态同步\n",
    "- 验证评估\n",
    "- 模型状态保存\n",
    "\n",
    "下面我们将分别对这几方面进行说明。\n",
    "\n",
    "#### 2.1 跨进程通信\n",
    "\n",
    "pytorch中提供了`torchrun`命令来启动分布式训练，启动命令类似`torchrun --nproc_per_node [GPU数量] xxx.py`，在这样的命令执行后，torchrun会自动为每个GPU进程设置环境变量，具体包括：\n",
    "- rank：在所有 node 的所有进程中，当前进程全局唯一编号，是从0开始计数的整数；\n",
    "- local rank：当前进程在本地node上的局部编号，也是从0开始计数的整数；\n",
    "- world size：所有并行进程的总数，各个 node 上并行的GPU总数；\n",
    "\n",
    "有了这些环境变量后，每个进程就可以在启动时知道全局并行的GPU进程总数量和当前的进程编号，这样就能通过一些协议来完成跨进程通信和状态同步。\n",
    "\n",
    "在分布式训练中，最常用的通信协议是NCCL（NVIDIA Collective Communications Library）, 它提供了以下通信原语来支持跨进程通信。\n",
    "- Broadcast: 将数据从一个 GPU 发送到所有其他 GPU。\n",
    "- All-Reduce: 所有进程的输入数据进行求和、求均值等操作，然后将结果返回给所有进程。\n",
    "- Reduce: 将所有进程的输入数据进行汇总（例如求和），只将结果发送给指定进程。\n",
    "- Send/Receive: 点对点的通信，直接在两个进程间传输数据。\n",
    "\n",
    "但这些通信原语并不需要我们手动调用，DDP会在每次反向传播后自动执行All-Reduce操作，以保证每个进程的模型参数保持同一状态，我们仅需要确保每个进程都被正确初始化。下面我们为训练器添加一个方法，用于完成分布式环境的初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21f1986-5eb4-48de-9b42-607b1ae5adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import init_process_group, destroy_process_group \n",
    "\n",
    "def init_distributed_mode(self):\n",
    "    rank = int(os.environ.get(\"RANK\", -1))\n",
    "    if rank == -1: \n",
    "        self.is_main_process = True\n",
    "        self.ddp = False\n",
    "        return\n",
    "    \n",
    "    os.environ['NCCL_DEBUG'] = 'WARN'\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    \n",
    "    self.ddp = True\n",
    "    self.rank = rank\n",
    "    self.is_main_process = self.rank == 0\n",
    "    self.local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    self.device = f'cuda:{local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "setattr(Trainer, \"init_distributed_mode\", init_distributed_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ed4d7-9802-4341-9c1d-cd8f07070822",
   "metadata": {},
   "source": [
    "这个方法主要为训练器初始化了以下几项：\n",
    "- ddp：是否启用分布式训练；\n",
    "- rank: 进程全局编号，用于通信和数据集切分；\n",
    "- local_rank：进程本地编号，用于定位GPU设备； \n",
    "- device：给当前进程分配的GPU设备,如`cuda:0`；\n",
    "- is_main_process: 用于定位当前进程是否属于主进程，主进程通常需要做一些特殊的操作；\n",
    "\n",
    "> 注：init_process_group用于将当前进程加入到nccl进程组中，加入到进程组中的进程可以互相进入跨进程通信。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85979351-72cc-4090-ad56-83665907ee4e",
   "metadata": {},
   "source": [
    "#### 2.2 数据集分片\n",
    "上面有提到，数据并行需要将数据集分割成多份，每份分配给一张GPU去训练。在DDP中，这个数据集切割和分配的功能由一个分布式采样器`DistributedSampler`来完成。具体来讲，我们需要做三点改造：\n",
    "1. 扩展方法以支持设置原始的训练集和测试集，而不是已经构建好的小批量数据加载器（dataloader)，这样就能对数据集作切分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6344ab05-39a0-45eb-bec6-34aab78c7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataset(self, train_set, eval_set):\n",
    "    self.train_set = train_set\n",
    "    self.eval_set = eval_set\n",
    "    print(f'set trainset: {len(train_set)}, evalset: {len(eval_set)}') if self.verbose else None\n",
    "\n",
    "setattr(Trainer, \"set_dataset\", set_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175cebf-49a4-4b7c-87a7-160f61c1aacb",
   "metadata": {},
   "source": [
    "2. 创建数据加载器时引入分布式采样器。具体来讲，是要创建一个分布式采样器实例sampler，并用它来构造训练集的加载器，表示训练数据将使用指定的采样器通过部分采样得到，而不是用整个训练数据集。\n",
    "\n",
    "> 注：每个GPU进程上都会使用采样器对训练数据集作切分和采样操作，它内部会自动根据world_size来决定将数据集切成多少份，使用local_rank来决定当前进程应该取哪份数据来训练。另外采样器与shuffle操作是互斥的，使用采样器后需要将shuffle关掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3cd7baf-c14c-40db-b4dc-5a18ce5c7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, DistributedSampler, random_split\n",
    "\n",
    "def init_dataloader(self):\n",
    "    assert self.train_set and self.eval_set, f\"train_set and eval_set can't be empty.\"\n",
    "    train_set, eval_set, batch_size = self.train_set, self.eval_set, self.batch_size\n",
    "    sampler = DistributedSampler(train_set) if self.ddp else None\n",
    "    self.train_loader = DataLoader(train_set, \n",
    "                                   batch_size=batch_size, \n",
    "                                   shuffle=(sampler==None), \n",
    "                                   num_workers=0, \n",
    "                                   drop_last=True, \n",
    "                                   sampler=sampler)\n",
    "    self.eval_loader = DataLoader(eval_set, \n",
    "                                  batch_size=batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=0, \n",
    "                                  drop_last=False)\n",
    "    self.steps_per_epoch = len(self.train_loader)\n",
    "    self.total_steps = self.num_epochs * self.steps_per_epoch\n",
    "    print(f'init train_loader steps: {len(self.train_loader)}, eval_loader: {len(self.eval_loader)}') if self.verbose else None\n",
    "\n",
    "setattr(Trainer, \"init_dataloader\", init_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892df94-53db-404e-a846-db3c3fb41a63",
   "metadata": {},
   "source": [
    "3. 动态分配：一个数据集往往需要训练多轮，DDP支持在每个轮训练开始前，通过`set_epoch`方法动态计算出数据集索引，这样可以确保每个epoch开始前都可以随机打乱数据顺序，相当于分布式下的shuffle操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300eeb3-65f0-438b-8a9a-ae893fcf11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(self, epoch):\n",
    "    assert self.train_loader and self.eval_loader, f\"train_loader and eval_loader can't be empty.\"\n",
    "    self.train_loader.sampler.set_epoch(epoch)\n",
    "    for i, (X, Y) in enumerate(self.train_loader): \n",
    "        ……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f64ba6-79e9-4ee3-a0d9-c5b6cf69f724",
   "metadata": {},
   "source": [
    "#### 2.3 模型复制\n",
    "在DDP的分布式训练中，每个GPU进程需要**复制**一份完整的模型参数和优化器状态。因此需要在各张卡上分别加载一份模型，并使用**DDP包装**，包装的目的是让模型在训练过程中能在各个进程间同步参数状态。\n",
    "\n",
    "我们在训练器中添加一个`wrap_model_with_ddp`方法，用于对模型实例进行DDP包装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14e25734-ac4e-4b0b-bcf6-b92e61127563",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def wrap_model_with_ddp(model, local_rank):\n",
    "    # 位置编码用的是复数，而nccl不支持复数形式，此变量并不要求在多进程中保持一致，所以暂时屏蔽对此变量的同步\n",
    "    model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n",
    "    model = DistributedDataParallel(model, device_ids=[local_rank])\n",
    "    print(f\"{cur_time()} packaged model with DDP in cuda:{local_rank}\")\n",
    "    return model\n",
    "\n",
    "setattr(Trainer, \"wrap_model_with_ddp\", wrap_model_with_ddp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4ebd3-a42a-42ee-97f9-02e799adb873",
   "metadata": {},
   "source": [
    "#### 2.4 模型评估\n",
    "在分布式训练中，各张卡上的模型状态始终是保持同步的，所以对模型损失的评估只需要在主进程上进行。同时，为了训练流程的简洁，我们将模型评估的检测逻辑独立成一个新方法`check_and_evaluate`，它的主要逻辑是：在满足设定的eval_steps的前提下，只有主进程才会进行评估验证，并记录训练指标数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2f0477b3-4d68-4b92-ba9e-0e1a37c58efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_evaluate(self, lr):\n",
    "    if (self.step + 1) % self.eval_steps != 0:\n",
    "        return\n",
    "\n",
    "    if self.is_main_process:\n",
    "        train_loss = self.train_loss_acc/self.eval_steps\n",
    "        eval_loss = self.evaluate()\n",
    "        print(f\"{self.cur_time()} lr={lr:.5f}, train_loss: {train_loss:.4f}, \"\n",
    "            + f\"eval_loss: {eval_loss:.4f}, steps: {self.step}/{self.total_steps}\"\n",
    "        )\n",
    "        self.train_loss_acc = 0\n",
    "        \n",
    "    dist.barrier() if self.ddp else None\n",
    "\n",
    "setattr(Trainer, \"check_and_evaluate\", check_and_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3b067-b32d-435b-a7d9-c0eef980899e",
   "metadata": {},
   "source": [
    "> 注：上面方法中的`dist.barrier`是为了插入一个同步屏障，起到多进程间训练状态同步的目的。原因在于evaluate方法是一个耗时操作，添加这句代码可以让其它进程等待主进程执行完模型评估再统一进入下一步训练，避免多个进程中的模型状态不同步，触发`nccl同步超时`。\n",
    "\n",
    "\n",
    "除此之外在评估阶段，我们需要直接访问模型的原始状态，而不是DDP封装，访问模型的原始状态可以用`model.module`。\n",
    "> 注：这里其实是一个坑，处理不好会造成训练过程中僵死，原因在于DDP封装过的模型在推理时会自动在多卡之间同步运算状态，但由于其它进程并没有运行模型评估，所以这个状态永远同步不到，训练时的表现就是整个训练进度卡住不动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2b9b9-a0ef-4449-9e4e-df1ca52cd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self):\n",
    "    # 注意：这里不能多进程同步，必须用原始Model\n",
    "    model = self.model.module if isinstance(self.model, DistributedDataParallel) else self.model\n",
    "    model.eval()\n",
    "    num_batches = len(self.eval_loader)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (X, Y) in self.eval_loader:\n",
    "        with torch.no_grad():\n",
    "            logits = model(X.to(self.device))\n",
    "        loss = f.cross_entropy(logits.flatten(0, 1), Y.to(self.device).flatten())\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss/num_batches  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943c032-ac11-4e9f-9b8e-b6487d39c2bb",
   "metadata": {},
   "source": [
    "在分布式训练中，各个进程间的模型参数状态和优化器状态，需要始终保持同步。正常情况下DDP会自动处理各个进程间的训练状态同步，但是像上面这种只需要在主进程上运行的`evaluate`操作，我们就必须手动加一个`barrier`操作来处理同步，否则主进程的训练进度就会逐渐落后于其它进程，并且训练的时间越长，模型评估执行的次数越多，落后的进度就越大，最终会超过nccl的多进程同步超时时间而触发`NCCL timeout`。\n",
    "\n",
    "\n",
    "#### 2.5 模型保存与恢复 \n",
    "\n",
    "除上面的evaluate操作外，模型的状态保存也只需要在主进程上执行。因此，我们将模型保存的检测逻辑也独立成一个新方法`check_and_save_checkpoint`，它的主要逻辑是：在满足设定的save_steps的前提下，只有主进程才会保存训练的checkpoint，其它进程需要在`dist.barrier()`这行代码处等待主进程完成保存操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fc5039a8-42ba-4d5c-a0fc-52818388a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_save_checkpoint(self, cur_epoch):\n",
    "    if self.step % self.save_steps != 0:\n",
    "        return\n",
    "    \n",
    "    if self.is_main_process:\n",
    "        checkpoint_path = f\"{self.output_dir}/checkpoint-{self.step}.pth\"\n",
    "        self.save_model(checkpoint_path, cur_epoch)\n",
    "        print(f\"{self.cur_time()} device:{self.device}-save checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "    # 设置屏障, 让所有进程等待主进程的checkpoint操作\n",
    "    dist.barrier() if self.ddp else None  \n",
    "\n",
    "setattr(Trainer, \"check_and_save_checkpoint\", check_and_save_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5171380-08f5-4584-8066-c04e314ecfc8",
   "metadata": {},
   "source": [
    "## 3. 训练流程改造\n",
    "\n",
    "上面增加的环节需要在训练流程中支持才能生效，我们需要分别对单轮训练函数和主训练函数进行改造。\n",
    "\n",
    "#### 3.1 单轮训练改造\n",
    "\n",
    "对于单轮训练函数，调整和加入了以下步骤：\n",
    "1. 分布式模式下，在单轮训练开始前用set_epoch函数打乱顺序； \n",
    "2. 每次单步训练前，调用`adjust_lr`动态调整学习率；\n",
    "3. 评估验证改用封装后的`check_and_evaluate`方法，兼容单卡训练和多卡训练；\n",
    "4. 增加保存checkpoint环节的函数调用：`check_and_save_checkpoint`，也兼容单卡和多卡训练； "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b33d381a-da91-47ca-994d-59cc36d41337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(self, cur_epoch):\n",
    "    assert self.train_loader and self.eval_loader, f\"train_loader and eval_loader can't be empty.\"\n",
    "    # 每个epoch开始时都重新打乱数据\n",
    "    self.train_loader.sampler.set_epoch(cur_epoch) if self.ddp else None  \n",
    "    \n",
    "    for i, (X, Y) in enumerate(self.train_loader):     \n",
    "        lr = self.adjust_lr()\n",
    "        train_loss = self.train_step(X.to(self.device), Y.to(self.device))\n",
    "        self.train_loss_acc += train_loss.item()\n",
    "        self.step += 1\n",
    "        self.check_and_evaluate(lr)\n",
    "        self.check_and_save_checkpoint(cur_epoch)\n",
    "\n",
    "setattr(Trainer, \"train_epoch\", train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec074845-b97d-4a21-83ea-549d9fb75221",
   "metadata": {},
   "source": [
    "#### 3.2 主训练改造\n",
    "\n",
    "对于主训练函数，重构流程如下：\n",
    "1. 添加对分布式环境的初始化； \n",
    "2. 添加对恢复训练的支持，能从指定的checkpoint恢复训练； \n",
    "3. 添加对DDP模型状态同步的支持； \n",
    "4. 主循环基本不变，只添加了从上次epoch继续训练的支持； \n",
    "5. 最后训练完后，清理和注销进程资源； "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9fbd1362-9217-48b1-99c2-6b9c4878a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    # 初始化分布式环境\n",
    "    self.init_distributed_mode()\n",
    "    # 初始化数据加载器\n",
    "    self.init_dataloader()\n",
    "    # 将模型移到指定设备\n",
    "    self.model.to(self.device)\n",
    "    # 恢复训练状态\n",
    "    last_epoch = 0\n",
    "    if self.last_checkpoint_path:\n",
    "        last_epoch = self.load_from_checkpoint()\n",
    "    # 分布式训练需要使用ddp同步模型状态\n",
    "    if self.ddp:\n",
    "        self.model = self.wrap_model_with_ddp(self.model, self.local_rank)\n",
    "    # 打印模型所在的设备  \n",
    "    model_device = next(self.model.parameters()).device  \n",
    "    print(\"Model is on device: \", model_device)  \n",
    "    # 训练主循环\n",
    "    for epoch in range(last_epoch, self.num_epochs):\n",
    "        self.train_epoch(epoch)\n",
    "    # 注销分布式进程\n",
    "    dist.destroy_process_group() if self.ddp else None\n",
    "\n",
    "setattr(Trainer, \"train\", train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d49364-8de0-4f58-943b-2a2be501e2e8",
   "metadata": {},
   "source": [
    "#### 3.3 main函数改造\n",
    "\n",
    "编写main函数，用于启动模型训练，主要职责如下：\n",
    "1. 设置训练参数\n",
    "2. 配置模型参数的输入/输出路径，用于恢复和保存模型；\n",
    "3. 创建模型实例，并配置优化器；\n",
    "4. 加载数据集，并按照比例切分为训练集和测试集；\n",
    "5. 创建和初始化训练器，并开始训练； "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c15dec8-7313-415a-b2bb-31b9db6e8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    epochs = 1\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 16\n",
    "    train_ratio = 0.98\n",
    "    weight_decay = 0.01\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    last_checkpoint_path = \"\"\n",
    "    dataset_path = \"/data2/minigpt/dataset/pretrain/mobvoi_seq_monkey_general_open_corpus.bin\"\n",
    "    output_dir = \"/data2/minigpt/models/20241210\"\n",
    "\n",
    "    config = GPTConfig(flash_attn=True)\n",
    "    model = MiniGPT(config)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    ds = PretrainBinaryDataset(dataset_path, config.context_length)\n",
    "    train_set, eval_set = split_dataset(ds[:100000], train_ratio)\n",
    "\n",
    "    train_args = {\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"eval_steps\": 500,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"save_steps\": 12000,\n",
    "        \"num_train_epochs\": epochs,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"last_checkpoint_path\": last_checkpoint_path,\n",
    "        \"use_mixed_precision\": True,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(model, optimizer, train_args, device=device, verbose=True)\n",
    "    trainer.set_seed(123)\n",
    "    trainer.set_dataset(train_set, eval_set)\n",
    "    trainer.set_grad_scaler(True)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ac599-c48c-4e28-aedf-b912c98fe0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "至此，分布式训练的代码部分就基本编写完成，我们将上面编写的整个训练器+main函数代码保存到一个名为`pretrainer.py`脚本中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2764f1-638e-4b48-a385-c864da069688",
   "metadata": {},
   "source": [
    "## 4. 分布式训练\n",
    "\n",
    "分布式训练需要使用多张卡，我们可以先用`nvidia-smi`命令看一下机器上有哪些卡可以使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb2abb0-144b-4f26-b484-0ba173dcc325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  1 08:04:42 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090         Off| 00000000:3E:00.0 Off |                  N/A |\n",
      "| 30%   22C    P8               26W / 350W|      2MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090         Off| 00000000:40:00.0 Off |                  N/A |\n",
      "| 30%   22C    P8               18W / 350W|      2MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090         Off| 00000000:41:00.0 Off |                  N/A |\n",
      "| 30%   23C    P8               22W / 350W|      2MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090         Off| 00000000:B1:00.0 Off |                  N/A |\n",
      "| 30%   20C    P8               20W / 350W|  20498MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 3090         Off| 00000000:B2:00.0 Off |                  N/A |\n",
      "| 30%   19C    P8                6W / 350W|   5554MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 3090         Off| 00000000:B5:00.0 Off |                  N/A |\n",
      "| 30%   23C    P8               14W / 350W|      2MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    3   N/A  N/A      6044      C   ...naconda3/envs/python3_10/bin/python    20482MiB |\n",
      "|    4   N/A  N/A     48022      C   ...naconda3/envs/python3_10/bin/python     5538MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25638a2e-fadd-4ade-be54-5e176d032714",
   "metadata": {},
   "source": [
    "我们这里通过环境变量`CUDA_VISIBLE_DEVICES`来指定要使用的GPU卡号，使用`torchrun`运行`pretrainer.py`脚本启动训练。\n",
    "> 注意：下面列出的命令只是示意，`CUDA_VISIBLE_DEVICES=0,1,2`在jupyter notebook中是不起作用的，我们需要从终端运行下面的命令,才能真正启动多个进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce61442-96fc-400a-b85e-88e4dc133661",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0,1,2 torchrun --nproc_per_node 3 pretrainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cde823-d031-4607-9e23-e1a59306e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "set trainset: 98000, evalset: 2000\n",
    "init train_loader steps: 2041, eval_loader: 125\n",
    "2024-12-28 07:33:12 start epoch:0 from step:0\n",
    "2024-12-28 07:35:16 lr=0.00099, train_loss: 7.3078, eval_loss: 6.4596, grad_norm=0.37541, steps: 199/2041\n",
    "2024-12-28 07:37:20 lr=0.00097, train_loss: 6.2862, eval_loss: 6.1645, grad_norm=0.24622, steps: 399/2041\n",
    "2024-12-28 07:39:23 lr=0.00090, train_loss: 6.1241, eval_loss: 6.0611, grad_norm=0.23912, steps: 599/2041\n",
    "2024-12-28 07:41:27 lr=0.00079, train_loss: 6.0449, eval_loss: 6.0008, grad_norm=0.21517, steps: 799/2041\n",
    "2024-12-28 07:43:30 lr=0.00064, train_loss: 5.9862, eval_loss: 5.9592, grad_norm=0.20768, steps: 999/2041\n",
    "2024-12-28 07:45:37 lr=0.00049, train_loss: 5.9465, eval_loss: 5.9235, grad_norm=0.20683, steps: 1199/2041\n",
    "2024-12-28 07:47:40 lr=0.00034, train_loss: 5.9102, eval_loss: 5.8910, grad_norm=0.19565, steps: 1399/2041\n",
    "2024-12-28 07:49:44 lr=0.00022, train_loss: 5.8823, eval_loss: 5.8677, grad_norm=0.19772, steps: 1599/2041\n",
    "2024-12-28 07:51:48 lr=0.00014, train_loss: 5.8657, eval_loss: 5.8471, grad_norm=0.20222, steps: 1799/2041\n",
    "2024-12-28 07:53:51 lr=0.00010, train_loss: 5.8473, eval_loss: 5.8367, grad_norm=0.19727, steps: 1999/2041\n",
    "2024-12-28 07:53:54 device:cuda:0-save checkpoint: /data2/minigpt/models/20241015/checkpoint-2000.pth\n",
    "2024-12-28 07:53:54 barrier wait over of device:cuda:0 at step: 2000.\n",
    "2024-12-28 07:53:54 barrier wait over of device:cuda:2 at step: 2000.\n",
    "2024-12-28 07:53:54 barrier wait over of device:cuda:1 at step: 2000.\n",
    "clean multi process.\n",
    "train over, steps: 2041\n",
    "train use time: 21.10min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2cdecd-424e-497d-9708-1d90c29db33b",
   "metadata": {},
   "source": [
    "> 从上面的训练日志来看，整个训练只用了21分钟，训练数据则与之前相同依然是10万条，相比上一节单卡优化的训练速度50分钟又提高了`2.38倍`，而相比上上一节预训练从零起步的训练速度128分钟则提高了`6.1倍`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3ae7c-0e5c-4121-af26-d1693751269f",
   "metadata": {},
   "source": [
    "上面只是用了一个10万的迷你数据集进行训练演示，实际中我们肯定要对整个预训练数据集进行完整的训练。我这边实践过的经历，是对这个预训练数据集进行了两轮训练，在3张24G显存的卡上跑了约70小时，训练损失和验证损失分别下降到了2.97和2.88左右，但整个训练过程比较漫长，不太方便展开。训练完后用以下函数进行推理测试的效果如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fa83d-cff1-463c-b468-0249307358bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    device='cuda:5'\n",
    "    model = MiniGPT(GPTConfig(flash_attn=False)).to(device)\n",
    "    checkpoint_path = \"/data2/minigpt/models/20241210/checkpoint-450000.pth\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    \n",
    "    tokenizer_path = \"/data2/minigpt/models/tokenizer_v3\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)\n",
    "\n",
    "    input_text = \"库里在第三节上篮时被防守球员犯规，但裁判并未理会\"\n",
    "    inputs = torch.tensor([tokenizer.encode(input_text)]).to(device)\n",
    "    response_ids = model.generate(inputs, max_length=512, eos_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.decode(response_ids.squeeze(0), skip_special_tokens=False))\n",
    "\n",
    "predict_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c6349-60fb-4ea1-8655-06c1003dc20c",
   "metadata": {},
   "source": [
    "> 库里在第三节上篮时被防守球员犯规，但裁判并未理会。第4节，在第9分46秒，维尔德在三分线外接到队友传球后，> 在抢断球后回身抽射，皮球被库里扑出。第10分54秒，维尔德在安德森的助攻下，在禁区内手抛球，裁判判罚点球。> 第11分45秒，维尔德在安德森的助攻下，在禁区内手抛球，裁判判罚点球。第13分46秒，维尔德在禁区内手抛球，> 裁判判罚点球。第15分47秒，维尔德在禁区内手抛球，裁判判罚点球。第16分46秒，维尔德在禁区内手抛球，裁判> 判罚点球。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c2c0b-92d1-4369-9d89-65bd45049033",
   "metadata": {},
   "source": [
    "**小结**：本文主要介绍了使用DDP进行分布式训练的技术原理和方法，DDP主要采用模型复制、数据并行再结合多进程通信来实现分布式训练。我们先对模型、数据、评估等基础环节进行了改造，之后又对整个训练流程进行分布式改造，最后实际启动脚本进行了分布式训练演示。\n",
    "\n",
    "总体来讲，多卡并行对训练速度的提升效果非常明显，原理也容易理解，不过分布式训练具体实施过程则相比于单卡训练要复杂很多，特别是涉及到多进程同步的部分，特别容易出问题，需要仔细理解来弄懂其背后的原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577943f-0afd-4ed5-8b1e-9f6b26bfdafc",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "- [pytorch多卡并行](https://blog.csdn.net/wxc971231/article/details/132827787)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
